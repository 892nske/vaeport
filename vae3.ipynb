{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49271ba7-f2ca-4a37-958c-bd41d57f1b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-12 14:59:18.398965: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-12 14:59:18.670542: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-12 14:59:18.670575: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-12 14:59:20.194518: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-12 14:59:20.194741: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-12 14:59:20.194767: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/nske/miniconda3/envs/jp38/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82074929-2272-4d3c-b558-489245b6e4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "funds = [{'fund_cd': '2008010903', 'fund_name': 'SMT TOPIXインデックス･オープン', 'asset_class': '国内株式'}, \n",
    "        {'fund_cd': '2008010904', 'fund_name': 'SMT グローバル株式インデックス', 'asset_class': '先進国株式'},\n",
    "        {'fund_cd': '2008040102', 'fund_name': 'インデックスF海外新興国株式', 'asset_class': '新興国株式'},\n",
    "        {'fund_cd': '2008010905', 'fund_name': 'SMT 国内債券インデックス･オープン', 'asset_class': '国内債券'},\n",
    "        {'fund_cd': '2008010906', 'fund_name': 'SMT グローバル債券インデックス', 'asset_class': '先進国債券'},\n",
    "        {'fund_cd': '2008040103', 'fund_name': 'インデックスF海外新興国債券(1年決算型)', 'asset_class': '新興国債券'}, \n",
    "        {'fund_cd': '2008010907', 'fund_name': 'SMT J-REITインデックス･オープン', 'asset_class': '国内リート'},\n",
    "        {'fund_cd': '2008010908', 'fund_name': 'SMT グローバルREITインデックス', 'asset_class': '先進国リート'}\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be6bd34c-e68d-40cb-a7c9-0d72737210e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fundsdata = pd.read_csv('csv/'+funds[0]['fund_cd']+'.csv')\n",
    "fundsdata = fundsdata.drop('基準価額', axis=1)\n",
    "for fund in funds:\n",
    "    df = pd.read_csv('csv/'+fund['fund_cd']+'.csv')\n",
    "    df = df.rename(columns={'基準価額': fund['asset_class']})\n",
    "    fundsdata = pd.merge(fundsdata, df, on='日付')\n",
    "fundsdata.columns = ['date','jp_stock','foreign_stock','emerging_stock','jp_bond','foreign_bond','emerging_bond','jp_reit','foreign_reit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfbea29c-a674-4e5c-b563-bfbecac086f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2008~2019年をテストデータにする\n",
    "funds_test = fundsdata.query('date < 20200101')\n",
    "funds_dif = funds_test[ ['jp_stock','foreign_stock','emerging_stock','jp_bond','foreign_bond','emerging_bond','jp_reit','foreign_reit'] ]\n",
    "funds_dif = np.log(funds_dif)-np.log(funds_dif.shift(1))\n",
    "funds_dif = funds_dif.loc[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caf1b3e3-345f-4a75-9058-cfef7e0aedb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ウィンドウ区間の変化率をndarrayに変換\n",
    "w=30\n",
    "xlist = []\n",
    "for i in range(len(funds_dif.index) - w):\n",
    "    xlist.append(funds_dif.loc[i+1:i+w].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93009e40-a313-4768-ba0d-16febfa431f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.stack(xlist,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a447347b-b728-42c1-a2bb-6ca071cea074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# サンプリング層\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c7794dd-fc74-41a3-b368-30e9c881c1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-12 15:03:05.391291: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-02-12 15:03:05.391395: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-12 15:03:05.391435: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-UBLLJ3A): /proc/driver/nvidia/version does not exist\n",
      "2023-02-12 15:03:05.391832: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 30, 8, 1)]   0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 30, 1, 32)    288         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 30, 1, 64)    10304       ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 1920)         0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 16)           30736       ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " z_mean (Dense)                 (None, 2)            34          ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " z_log_var (Dense)              (None, 2)            34          ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " sampling (Sampling)            (None, 2)            0           ['z_mean[0][0]',                 \n",
      "                                                                  'z_log_var[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 41,396\n",
      "Trainable params: 41,396\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# VAEエンコーダ部分\n",
    "latent_dim = 2\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(w, 8, 1))\n",
    "x = layers.Conv2D(32, (1,8), activation=\"softplus\", padding=\"valid\")(encoder_inputs)\n",
    "x = layers.Conv1D(64, 5, activation=\"softplus\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(16, activation=\"softplus\")(x)\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cf0be9b-bfc8-4f8a-a5fb-e10572de9ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 2)]               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1920)              5760      \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 30, 1, 64)         0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTra  (None, 30, 1, 64)        20544     \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2DT  (None, 30, 8, 32)        16416     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2DT  (None, 30, 8, 1)         161       \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 42,881\n",
      "Trainable params: 42,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# VAEデコーダ部分\n",
    "latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "x = layers.Dense(30 * 1 * 64, activation=\"softplus\")(latent_inputs)\n",
    "x = layers.Reshape((30, 1, 64))(x)\n",
    "x = layers.Conv2DTranspose(64, (5,1), activation=\"softplus\", padding=\"same\")(x)\n",
    "x = layers.Conv2DTranspose(32, (1,8), activation=\"softplus\", padding=\"valid\")(x)\n",
    "decoder_outputs = layers.Conv2DTranspose(1, (5,1), activation=\"sigmoid\", padding=\"same\")(x)\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59dc4c9b-2378-4555-b517-729ff731f056",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "    \n",
    "    def predict(self, x):\n",
    "        z_mean, _, _ = self.encoder.predict(x)\n",
    "        y = self.decoder.predict(z_mean)\n",
    "        return y\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    keras.losses.binary_crossentropy(data, reconstruction), axis=(1, 2)\n",
    "                )\n",
    "            )\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8bb0057-66eb-49f0-ad25-906dd8f4afcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.expand_dims(x_test,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48e6bc6f-d366-4485-a8d7-2791a3b5c58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "23/23 [==============================] - 4s 51ms/step - loss: 55.2958 - reconstruction_loss: 22.0682 - kl_loss: 0.2251\n",
      "Epoch 2/500\n",
      "23/23 [==============================] - 1s 49ms/step - loss: 0.4675 - reconstruction_loss: 0.5520 - kl_loss: 0.0012\n",
      "Epoch 3/500\n",
      "23/23 [==============================] - 1s 46ms/step - loss: 0.7118 - reconstruction_loss: 0.5230 - kl_loss: 4.3268e-04\n",
      "Epoch 4/500\n",
      "23/23 [==============================] - 1s 48ms/step - loss: 0.4159 - reconstruction_loss: 0.4797 - kl_loss: 1.4837e-04\n",
      "Epoch 5/500\n",
      "23/23 [==============================] - 1s 47ms/step - loss: 0.3982 - reconstruction_loss: 0.4761 - kl_loss: 5.8811e-05\n",
      "Epoch 6/500\n",
      "23/23 [==============================] - 1s 45ms/step - loss: 0.4948 - reconstruction_loss: 0.4395 - kl_loss: 2.7125e-05\n",
      "Epoch 7/500\n",
      "23/23 [==============================] - 1s 47ms/step - loss: 0.3951 - reconstruction_loss: 0.4246 - kl_loss: 1.3393e-05\n",
      "Epoch 8/500\n",
      "23/23 [==============================] - 1s 46ms/step - loss: 0.5237 - reconstruction_loss: 0.4135 - kl_loss: 7.4559e-06\n",
      "Epoch 9/500\n",
      "23/23 [==============================] - 1s 45ms/step - loss: 0.3725 - reconstruction_loss: 0.4268 - kl_loss: 4.7142e-06\n",
      "Epoch 10/500\n",
      "23/23 [==============================] - 1s 45ms/step - loss: 0.3379 - reconstruction_loss: 0.4296 - kl_loss: 3.2086e-06\n",
      "Epoch 11/500\n",
      "23/23 [==============================] - 1s 47ms/step - loss: 0.4582 - reconstruction_loss: 0.3972 - kl_loss: 2.5013e-06\n",
      "Epoch 12/500\n",
      "23/23 [==============================] - 1s 49ms/step - loss: 0.3993 - reconstruction_loss: 0.3766 - kl_loss: 2.0558e-06\n",
      "Epoch 13/500\n",
      "23/23 [==============================] - 1s 51ms/step - loss: 0.5023 - reconstruction_loss: 0.4340 - kl_loss: 1.6746e-06\n",
      "Epoch 14/500\n",
      "23/23 [==============================] - 1s 48ms/step - loss: 0.4989 - reconstruction_loss: 0.4152 - kl_loss: 1.2501e-06\n",
      "Epoch 15/500\n",
      "23/23 [==============================] - 1s 48ms/step - loss: 0.4784 - reconstruction_loss: 0.4364 - kl_loss: 1.0859e-06\n",
      "Epoch 16/500\n",
      "23/23 [==============================] - 1s 50ms/step - loss: 0.3947 - reconstruction_loss: 0.4028 - kl_loss: 1.0675e-06\n",
      "Epoch 17/500\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 0.4831 - reconstruction_loss: 0.3452 - kl_loss: 8.4156e-07\n",
      "Epoch 18/500\n",
      "23/23 [==============================] - 2s 67ms/step - loss: 0.3681 - reconstruction_loss: 0.4112 - kl_loss: 4.8722e-07\n",
      "Epoch 19/500\n",
      "23/23 [==============================] - 2s 67ms/step - loss: 0.3940 - reconstruction_loss: 0.4324 - kl_loss: 1.0347e-06\n",
      "Epoch 20/500\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 0.4410 - reconstruction_loss: 0.3946 - kl_loss: 1.1777e-06\n",
      "Epoch 21/500\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 0.4282 - reconstruction_loss: 0.3925 - kl_loss: 8.5246e-07\n",
      "Epoch 22/500\n",
      "23/23 [==============================] - 1s 65ms/step - loss: 0.3757 - reconstruction_loss: 0.3598 - kl_loss: 7.0950e-07\n",
      "Epoch 23/500\n",
      "23/23 [==============================] - 2s 97ms/step - loss: 0.4577 - reconstruction_loss: 0.3513 - kl_loss: 2.9699e-07\n",
      "Epoch 24/500\n",
      "23/23 [==============================] - 2s 92ms/step - loss: 0.3893 - reconstruction_loss: 0.3522 - kl_loss: 1.0538e-07\n",
      "Epoch 25/500\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.3336 - reconstruction_loss: 0.3663 - kl_loss: 2.7961e-07\n",
      "Epoch 26/500\n",
      "23/23 [==============================] - 3s 129ms/step - loss: 0.3458 - reconstruction_loss: 0.4003 - kl_loss: 2.3689e-07\n",
      "Epoch 27/500\n",
      "23/23 [==============================] - 2s 82ms/step - loss: 0.3181 - reconstruction_loss: 0.4090 - kl_loss: 2.6364e-07\n",
      "Epoch 28/500\n",
      "23/23 [==============================] - 2s 65ms/step - loss: 0.4781 - reconstruction_loss: 0.4099 - kl_loss: 4.3567e-07\n",
      "Epoch 29/500\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 0.3663 - reconstruction_loss: 0.3825 - kl_loss: 3.6199e-07\n",
      "Epoch 30/500\n",
      "23/23 [==============================] - 2s 93ms/step - loss: 0.4001 - reconstruction_loss: 0.4320 - kl_loss: 4.7979e-07\n",
      "Epoch 31/500\n",
      "23/23 [==============================] - 1s 50ms/step - loss: 0.3473 - reconstruction_loss: 0.3778 - kl_loss: 5.8203e-07\n",
      "Epoch 32/500\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.4431 - reconstruction_loss: 0.3671 - kl_loss: 5.2394e-07\n",
      "Epoch 33/500\n",
      "23/23 [==============================] - 3s 116ms/step - loss: 0.3511 - reconstruction_loss: 0.3870 - kl_loss: 4.4080e-07\n",
      "Epoch 34/500\n",
      "23/23 [==============================] - 2s 98ms/step - loss: 0.4028 - reconstruction_loss: 0.4013 - kl_loss: 3.6282e-07\n",
      "Epoch 35/500\n",
      "23/23 [==============================] - 1s 53ms/step - loss: 0.3687 - reconstruction_loss: 0.4366 - kl_loss: 3.9181e-07\n",
      "Epoch 36/500\n",
      "23/23 [==============================] - 1s 42ms/step - loss: 0.4156 - reconstruction_loss: 0.3216 - kl_loss: 2.3298e-07\n",
      "Epoch 37/500\n",
      "23/23 [==============================] - 1s 42ms/step - loss: 0.4417 - reconstruction_loss: 0.3149 - kl_loss: 1.7241e-07\n",
      "Epoch 38/500\n",
      "23/23 [==============================] - 2s 92ms/step - loss: 0.4498 - reconstruction_loss: 0.3755 - kl_loss: 4.7465e-07\n",
      "Epoch 39/500\n",
      "23/23 [==============================] - 3s 127ms/step - loss: 0.3020 - reconstruction_loss: 0.4133 - kl_loss: 4.1800e-07\n",
      "Epoch 40/500\n",
      "23/23 [==============================] - 2s 72ms/step - loss: 0.3627 - reconstruction_loss: 0.3613 - kl_loss: 1.0654e-07\n",
      "Epoch 41/500\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 0.3362 - reconstruction_loss: 0.4122 - kl_loss: 2.3817e-07\n",
      "Epoch 42/500\n",
      "23/23 [==============================] - 2s 64ms/step - loss: 0.3245 - reconstruction_loss: 0.3307 - kl_loss: 6.7585e-07\n",
      "Epoch 43/500\n",
      "23/23 [==============================] - 2s 75ms/step - loss: 0.3691 - reconstruction_loss: 0.3676 - kl_loss: 1.8330e-07\n",
      "Epoch 44/500\n",
      "23/23 [==============================] - 2s 94ms/step - loss: 0.3628 - reconstruction_loss: 0.3239 - kl_loss: 4.9684e-07\n",
      "Epoch 45/500\n",
      "23/23 [==============================] - 3s 119ms/step - loss: 0.4202 - reconstruction_loss: 0.3324 - kl_loss: 1.3406e-06\n",
      "Epoch 46/500\n",
      "23/23 [==============================] - 2s 92ms/step - loss: 0.3642 - reconstruction_loss: 0.3900 - kl_loss: 1.1128e-06\n",
      "Epoch 47/500\n",
      "23/23 [==============================] - 2s 96ms/step - loss: 0.2965 - reconstruction_loss: 0.3690 - kl_loss: 5.4538e-07\n",
      "Epoch 48/500\n",
      "23/23 [==============================] - 2s 71ms/step - loss: 0.3688 - reconstruction_loss: 0.3202 - kl_loss: 1.0151e-06\n",
      "Epoch 49/500\n",
      "23/23 [==============================] - 2s 67ms/step - loss: 0.3042 - reconstruction_loss: 0.3493 - kl_loss: 6.7050e-07\n",
      "Epoch 50/500\n",
      "23/23 [==============================] - 2s 73ms/step - loss: 0.3703 - reconstruction_loss: 0.3393 - kl_loss: 1.1175e-06\n",
      "Epoch 51/500\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.3176 - reconstruction_loss: 0.3352 - kl_loss: 5.8734e-07\n",
      "Epoch 52/500\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 0.4148 - reconstruction_loss: 0.3617 - kl_loss: 2.4708e-06\n",
      "Epoch 53/500\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 0.3169 - reconstruction_loss: 0.3978 - kl_loss: 8.9496e-07\n",
      "Epoch 54/500\n",
      "23/23 [==============================] - 1s 53ms/step - loss: 0.3998 - reconstruction_loss: 0.3218 - kl_loss: 2.7249e-06\n",
      "Epoch 55/500\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 0.3221 - reconstruction_loss: 0.3327 - kl_loss: 1.6704e-06\n",
      "Epoch 56/500\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 0.3801 - reconstruction_loss: 0.3690 - kl_loss: 1.9745e-06\n",
      "Epoch 57/500\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 0.3845 - reconstruction_loss: 0.3899 - kl_loss: 1.4904e-06\n",
      "Epoch 58/500\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 0.4170 - reconstruction_loss: 0.3542 - kl_loss: 1.4597e-06\n",
      "Epoch 59/500\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 0.3789 - reconstruction_loss: 0.3564 - kl_loss: 1.1552e-06\n",
      "Epoch 60/500\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 0.3809 - reconstruction_loss: 0.3817 - kl_loss: 1.2557e-06\n",
      "Epoch 61/500\n",
      "23/23 [==============================] - 2s 80ms/step - loss: 0.2202 - reconstruction_loss: 0.4012 - kl_loss: 2.2774e-06\n",
      "Epoch 62/500\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 0.3836 - reconstruction_loss: 0.3559 - kl_loss: 8.0629e-06\n",
      "Epoch 63/500\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 0.3444 - reconstruction_loss: 0.3294 - kl_loss: 3.7705e-06\n",
      "Epoch 64/500\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 0.2863 - reconstruction_loss: 0.3641 - kl_loss: 1.0397e-06\n",
      "Epoch 65/500\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 0.4470 - reconstruction_loss: 0.3672 - kl_loss: 6.0761e-06\n",
      "Epoch 66/500\n",
      "23/23 [==============================] - 2s 68ms/step - loss: 0.3774 - reconstruction_loss: 0.3510 - kl_loss: 8.1664e-07\n",
      "Epoch 67/500\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 0.4884 - reconstruction_loss: 0.3120 - kl_loss: 2.0991e-06\n",
      "Epoch 68/500\n",
      "23/23 [==============================] - 2s 70ms/step - loss: 0.4210 - reconstruction_loss: 0.2944 - kl_loss: 2.0196e-06\n",
      "Epoch 69/500\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 0.2526 - reconstruction_loss: 0.3372 - kl_loss: 2.5887e-06\n",
      "Epoch 70/500\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.2519 - reconstruction_loss: 0.3884 - kl_loss: 4.8800e-06\n",
      "Epoch 71/500\n",
      "23/23 [==============================] - 2s 88ms/step - loss: 0.4334 - reconstruction_loss: 0.3166 - kl_loss: 8.9891e-06\n",
      "Epoch 72/500\n",
      "23/23 [==============================] - 2s 98ms/step - loss: 0.3663 - reconstruction_loss: 0.3491 - kl_loss: 4.5541e-06\n",
      "Epoch 73/500\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.4174 - reconstruction_loss: 0.3219 - kl_loss: 4.6239e-07\n",
      "Epoch 74/500\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 0.2924 - reconstruction_loss: 0.3960 - kl_loss: 7.9720e-07\n",
      "Epoch 75/500\n",
      "23/23 [==============================] - 2s 67ms/step - loss: 0.4779 - reconstruction_loss: 0.3073 - kl_loss: 2.9167e-06\n",
      "Epoch 76/500\n",
      "23/23 [==============================] - 1s 64ms/step - loss: 0.4299 - reconstruction_loss: 0.3535 - kl_loss: 1.7437e-06\n",
      "Epoch 77/500\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 0.3682 - reconstruction_loss: 0.3500 - kl_loss: 2.9399e-06\n",
      "Epoch 78/500\n",
      "23/23 [==============================] - 2s 72ms/step - loss: 0.2365 - reconstruction_loss: 0.3498 - kl_loss: 1.2723e-06\n",
      "Epoch 79/500\n",
      "23/23 [==============================] - 2s 69ms/step - loss: 0.4056 - reconstruction_loss: 0.3586 - kl_loss: 1.9306e-06\n",
      "Epoch 80/500\n",
      "23/23 [==============================] - 2s 84ms/step - loss: 0.3503 - reconstruction_loss: 0.3481 - kl_loss: 5.3512e-06\n",
      "Epoch 81/500\n",
      "23/23 [==============================] - 1s 64ms/step - loss: 0.3625 - reconstruction_loss: 0.3510 - kl_loss: 5.6220e-07\n",
      "Epoch 82/500\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.3251 - reconstruction_loss: 0.3539 - kl_loss: 2.9812e-07\n",
      "Epoch 83/500\n",
      "23/23 [==============================] - 2s 73ms/step - loss: 0.3394 - reconstruction_loss: 0.3710 - kl_loss: 6.2071e-07\n",
      "Epoch 84/500\n",
      "23/23 [==============================] - 2s 67ms/step - loss: 0.4348 - reconstruction_loss: 0.3600 - kl_loss: 2.1680e-07\n",
      "Epoch 85/500\n",
      "23/23 [==============================] - 1s 65ms/step - loss: 0.2888 - reconstruction_loss: 0.3385 - kl_loss: 8.8851e-07\n",
      "Epoch 86/500\n",
      "23/23 [==============================] - 1s 41ms/step - loss: 0.3067 - reconstruction_loss: 0.3277 - kl_loss: 2.3160e-06\n",
      "Epoch 87/500\n",
      "23/23 [==============================] - 1s 41ms/step - loss: 0.2734 - reconstruction_loss: 0.3658 - kl_loss: 3.7405e-06\n",
      "Epoch 88/500\n",
      "23/23 [==============================] - 1s 40ms/step - loss: 0.3750 - reconstruction_loss: 0.3467 - kl_loss: 4.6268e-06\n",
      "Epoch 89/500\n",
      "23/23 [==============================] - 1s 41ms/step - loss: 0.5014 - reconstruction_loss: 0.3179 - kl_loss: 3.3954e-06\n",
      "Epoch 90/500\n",
      "23/23 [==============================] - 1s 43ms/step - loss: 0.3494 - reconstruction_loss: 0.3687 - kl_loss: 5.7518e-06\n",
      "Epoch 91/500\n",
      "23/23 [==============================] - 1s 40ms/step - loss: 0.3749 - reconstruction_loss: 0.3095 - kl_loss: 3.6688e-06\n",
      "Epoch 92/500\n",
      "23/23 [==============================] - 1s 40ms/step - loss: 0.3991 - reconstruction_loss: 0.3441 - kl_loss: 1.4368e-06\n",
      "Epoch 93/500\n",
      "23/23 [==============================] - 1s 40ms/step - loss: 0.2443 - reconstruction_loss: 0.3404 - kl_loss: 1.4297e-06\n",
      "Epoch 94/500\n",
      "23/23 [==============================] - 1s 41ms/step - loss: 0.2911 - reconstruction_loss: 0.3640 - kl_loss: 2.5826e-06\n",
      "Epoch 95/500\n",
      "23/23 [==============================] - 1s 41ms/step - loss: 0.4581 - reconstruction_loss: 0.3235 - kl_loss: 3.8239e-06\n",
      "Epoch 96/500\n",
      "23/23 [==============================] - 1s 40ms/step - loss: 0.4455 - reconstruction_loss: 0.3447 - kl_loss: 3.2349e-06\n",
      "Epoch 97/500\n",
      "23/23 [==============================] - 1s 40ms/step - loss: 0.4285 - reconstruction_loss: 0.3047 - kl_loss: 9.4452e-07\n",
      "Epoch 98/500\n",
      "23/23 [==============================] - 1s 40ms/step - loss: 0.3930 - reconstruction_loss: 0.3121 - kl_loss: 5.4071e-07\n",
      "Epoch 99/500\n",
      "23/23 [==============================] - 1s 41ms/step - loss: 0.2535 - reconstruction_loss: 0.3702 - kl_loss: 3.4761e-07\n",
      "Epoch 100/500\n",
      "23/23 [==============================] - 1s 43ms/step - loss: 0.3933 - reconstruction_loss: 0.3681 - kl_loss: 2.1349e-06\n",
      "Epoch 101/500\n",
      "23/23 [==============================] - 1s 41ms/step - loss: 0.4300 - reconstruction_loss: 0.3588 - kl_loss: 5.4102e-07\n",
      "Epoch 102/500\n",
      "23/23 [==============================] - 1s 40ms/step - loss: 0.3296 - reconstruction_loss: 0.3619 - kl_loss: 6.0199e-07\n",
      "Epoch 103/500\n",
      "23/23 [==============================] - 1s 40ms/step - loss: 0.3512 - reconstruction_loss: 0.3550 - kl_loss: 1.2019e-06\n",
      "Epoch 104/500\n",
      "23/23 [==============================] - 1s 40ms/step - loss: 0.2625 - reconstruction_loss: 0.3724 - kl_loss: 2.0169e-06\n",
      "Epoch 105/500\n",
      "23/23 [==============================] - 1s 40ms/step - loss: 0.3813 - reconstruction_loss: 0.3225 - kl_loss: 1.5355e-06\n",
      "Epoch 106/500\n",
      "23/23 [==============================] - 1s 41ms/step - loss: 0.2575 - reconstruction_loss: 0.3369 - kl_loss: 9.6977e-07\n",
      "Epoch 107/500\n",
      "23/23 [==============================] - 1s 41ms/step - loss: 0.2824 - reconstruction_loss: 0.3703 - kl_loss: 6.1120e-07\n",
      "Epoch 108/500\n",
      "23/23 [==============================] - 1s 40ms/step - loss: 0.3352 - reconstruction_loss: 0.3236 - kl_loss: 1.3685e-06\n",
      "Epoch 109/500\n",
      "23/23 [==============================] - 1s 40ms/step - loss: 0.2497 - reconstruction_loss: 0.3756 - kl_loss: 1.5411e-06\n",
      "Epoch 110/500\n",
      "23/23 [==============================] - 1s 40ms/step - loss: 0.3275 - reconstruction_loss: 0.3534 - kl_loss: 2.8435e-06\n",
      "Epoch 111/500\n",
      "23/23 [==============================] - 1s 43ms/step - loss: 0.3853 - reconstruction_loss: 0.3981 - kl_loss: 7.0489e-07\n",
      "Epoch 112/500\n",
      "23/23 [==============================] - 1s 40ms/step - loss: 0.3345 - reconstruction_loss: 0.3502 - kl_loss: 5.8723e-07\n",
      "Epoch 113/500\n",
      "23/23 [==============================] - 1s 40ms/step - loss: 0.3310 - reconstruction_loss: 0.3308 - kl_loss: 7.0034e-07\n",
      "Epoch 114/500\n",
      "23/23 [==============================] - 1s 41ms/step - loss: 0.3088 - reconstruction_loss: 0.3808 - kl_loss: 1.4304e-06\n",
      "Epoch 115/500\n",
      "23/23 [==============================] - 1s 42ms/step - loss: 0.3167 - reconstruction_loss: 0.3448 - kl_loss: 3.4522e-06\n",
      "Epoch 116/500\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 0.4212 - reconstruction_loss: 0.3229 - kl_loss: 4.5779e-06\n",
      "Epoch 117/500\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 0.3360 - reconstruction_loss: 0.3529 - kl_loss: 2.0284e-06\n",
      "Epoch 118/500\n",
      "23/23 [==============================] - 2s 65ms/step - loss: 0.2831 - reconstruction_loss: 0.3687 - kl_loss: 1.0939e-06\n",
      "Epoch 119/500\n",
      "23/23 [==============================] - 2s 67ms/step - loss: 0.2736 - reconstruction_loss: 0.3410 - kl_loss: 2.8315e-06\n",
      "Epoch 120/500\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 0.2843 - reconstruction_loss: 0.3441 - kl_loss: 2.3861e-06\n",
      "Epoch 121/500\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 0.4107 - reconstruction_loss: 0.3818 - kl_loss: 1.3583e-06\n",
      "Epoch 122/500\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 0.4516 - reconstruction_loss: 0.2873 - kl_loss: 1.1801e-06\n",
      "Epoch 123/500\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 0.2930 - reconstruction_loss: 0.3868 - kl_loss: 1.6104e-06\n",
      "Epoch 124/500\n",
      "23/23 [==============================] - 1s 64ms/step - loss: 0.3444 - reconstruction_loss: 0.3685 - kl_loss: 5.7888e-06\n",
      "Epoch 125/500\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 0.4642 - reconstruction_loss: 0.3311 - kl_loss: 5.6010e-07\n",
      "Epoch 126/500\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 0.3424 - reconstruction_loss: 0.3477 - kl_loss: 4.2294e-06\n",
      "Epoch 127/500\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 0.4577 - reconstruction_loss: 0.2833 - kl_loss: 1.4126e-06\n",
      "Epoch 128/500\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 0.3270 - reconstruction_loss: 0.3695 - kl_loss: 8.7151e-07\n",
      "Epoch 129/500\n",
      "23/23 [==============================] - 2s 65ms/step - loss: 0.4300 - reconstruction_loss: 0.3662 - kl_loss: 1.3380e-06\n",
      "Epoch 130/500\n",
      "23/23 [==============================] - 1s 63ms/step - loss: 0.3169 - reconstruction_loss: 0.3650 - kl_loss: 3.6153e-06\n",
      "Epoch 131/500\n",
      "23/23 [==============================] - 2s 70ms/step - loss: 0.3417 - reconstruction_loss: 0.3092 - kl_loss: 2.3297e-06\n",
      "Epoch 132/500\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 0.3624 - reconstruction_loss: 0.3217 - kl_loss: 3.0597e-06\n",
      "Epoch 133/500\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 0.2987 - reconstruction_loss: 0.3403 - kl_loss: 8.0534e-06\n",
      "Epoch 134/500\n",
      "23/23 [==============================] - 1s 63ms/step - loss: 0.3137 - reconstruction_loss: 0.3654 - kl_loss: 7.4697e-06\n",
      "Epoch 135/500\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 0.3603 - reconstruction_loss: 0.3379 - kl_loss: 9.3571e-06\n",
      "Epoch 136/500\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 0.2979 - reconstruction_loss: 0.3559 - kl_loss: 5.7315e-06\n",
      "Epoch 137/500\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 0.3522 - reconstruction_loss: 0.3167 - kl_loss: 7.6250e-06\n",
      "Epoch 138/500\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 0.3627 - reconstruction_loss: 0.3215 - kl_loss: 1.7366e-06\n",
      "Epoch 139/500\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 0.2140 - reconstruction_loss: 0.3301 - kl_loss: 2.8203e-06\n",
      "Epoch 140/500\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 0.3282 - reconstruction_loss: 0.3554 - kl_loss: 1.4382e-06\n",
      "Epoch 141/500\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 0.3928 - reconstruction_loss: 0.3139 - kl_loss: 3.2246e-06\n",
      "Epoch 142/500\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 0.3206 - reconstruction_loss: 0.3539 - kl_loss: 2.2605e-06\n",
      "Epoch 143/500\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 0.4114 - reconstruction_loss: 0.3673 - kl_loss: 6.4992e-06\n",
      "Epoch 144/500\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 0.2250 - reconstruction_loss: 0.3628 - kl_loss: 3.4102e-06\n",
      "Epoch 145/500\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 0.3334 - reconstruction_loss: 0.3711 - kl_loss: 2.2931e-06\n",
      "Epoch 146/500\n",
      "23/23 [==============================] - 2s 69ms/step - loss: 0.3248 - reconstruction_loss: 0.3594 - kl_loss: 3.1091e-06\n",
      "Epoch 147/500\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 0.3284 - reconstruction_loss: 0.3166 - kl_loss: 2.0220e-06\n",
      "Epoch 148/500\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 0.2806 - reconstruction_loss: 0.3505 - kl_loss: 1.5874e-06\n",
      "Epoch 149/500\n",
      "23/23 [==============================] - 2s 67ms/step - loss: 0.3111 - reconstruction_loss: 0.3421 - kl_loss: 2.5245e-06\n",
      "Epoch 150/500\n",
      "23/23 [==============================] - 2s 68ms/step - loss: 0.3362 - reconstruction_loss: 0.3418 - kl_loss: 5.3342e-06\n",
      "Epoch 151/500\n",
      "23/23 [==============================] - 1s 52ms/step - loss: 0.2638 - reconstruction_loss: 0.3738 - kl_loss: 9.6628e-06\n",
      "Epoch 152/500\n",
      "23/23 [==============================] - 1s 50ms/step - loss: 0.3814 - reconstruction_loss: 0.3314 - kl_loss: 6.8047e-07\n",
      "Epoch 153/500\n",
      "23/23 [==============================] - 1s 51ms/step - loss: 0.4220 - reconstruction_loss: 0.3709 - kl_loss: 1.0405e-06\n",
      "Epoch 154/500\n",
      "23/23 [==============================] - 1s 53ms/step - loss: 0.3906 - reconstruction_loss: 0.3572 - kl_loss: 1.3635e-05\n",
      "Epoch 155/500\n",
      "23/23 [==============================] - 1s 45ms/step - loss: 0.3967 - reconstruction_loss: 0.3072 - kl_loss: 6.2596e-06\n",
      "Epoch 156/500\n",
      "23/23 [==============================] - 1s 50ms/step - loss: 0.3034 - reconstruction_loss: 0.3299 - kl_loss: 9.8322e-07\n",
      "Epoch 157/500\n",
      "23/23 [==============================] - 1s 45ms/step - loss: 0.4296 - reconstruction_loss: 0.3368 - kl_loss: 4.4930e-06\n",
      "Epoch 158/500\n",
      "23/23 [==============================] - 1s 46ms/step - loss: 0.3897 - reconstruction_loss: 0.3567 - kl_loss: 3.7733e-06\n",
      "Epoch 159/500\n",
      "23/23 [==============================] - 1s 45ms/step - loss: 0.4109 - reconstruction_loss: 0.3386 - kl_loss: 3.1186e-06\n",
      "Epoch 160/500\n",
      "23/23 [==============================] - 1s 45ms/step - loss: 0.3961 - reconstruction_loss: 0.3202 - kl_loss: 2.9289e-06\n",
      "Epoch 161/500\n",
      "23/23 [==============================] - 1s 45ms/step - loss: 0.4451 - reconstruction_loss: 0.3246 - kl_loss: 1.3580e-06\n",
      "Epoch 162/500\n",
      "23/23 [==============================] - 1s 49ms/step - loss: 0.3101 - reconstruction_loss: 0.3716 - kl_loss: 1.2195e-06\n",
      "Epoch 163/500\n",
      "23/23 [==============================] - 1s 64ms/step - loss: 0.4022 - reconstruction_loss: 0.3228 - kl_loss: 1.6631e-06\n",
      "Epoch 164/500\n",
      "23/23 [==============================] - 2s 68ms/step - loss: 0.2984 - reconstruction_loss: 0.3500 - kl_loss: 3.6846e-06\n",
      "Epoch 165/500\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 0.3235 - reconstruction_loss: 0.3098 - kl_loss: 2.7713e-06\n",
      "Epoch 166/500\n",
      "23/23 [==============================] - 1s 48ms/step - loss: 0.3222 - reconstruction_loss: 0.2970 - kl_loss: 2.7119e-06\n",
      "Epoch 167/500\n",
      "23/23 [==============================] - 1s 50ms/step - loss: 0.3497 - reconstruction_loss: 0.3297 - kl_loss: 1.5738e-06\n",
      "Epoch 168/500\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 0.3531 - reconstruction_loss: 0.3562 - kl_loss: 4.1862e-06\n",
      "Epoch 169/500\n",
      "23/23 [==============================] - 1s 65ms/step - loss: 0.3062 - reconstruction_loss: 0.3656 - kl_loss: 1.1586e-06\n",
      "Epoch 170/500\n",
      "23/23 [==============================] - 2s 76ms/step - loss: 0.3683 - reconstruction_loss: 0.3755 - kl_loss: 6.1557e-06\n",
      "Epoch 171/500\n",
      "23/23 [==============================] - 2s 67ms/step - loss: 0.3763 - reconstruction_loss: 0.3495 - kl_loss: 1.3161e-06\n",
      "Epoch 172/500\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 0.3497 - reconstruction_loss: 0.3584 - kl_loss: 4.2861e-06\n",
      "Epoch 173/500\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.3707 - reconstruction_loss: 0.3438 - kl_loss: 9.4121e-06\n",
      "Epoch 174/500\n",
      "23/23 [==============================] - 2s 91ms/step - loss: 0.2255 - reconstruction_loss: 0.3596 - kl_loss: 2.8428e-06\n",
      "Epoch 175/500\n",
      "23/23 [==============================] - 2s 96ms/step - loss: 0.2601 - reconstruction_loss: 0.3448 - kl_loss: 3.0966e-06\n",
      "Epoch 176/500\n",
      "23/23 [==============================] - 2s 98ms/step - loss: 0.2644 - reconstruction_loss: 0.3118 - kl_loss: 5.7554e-06\n",
      "Epoch 177/500\n",
      "23/23 [==============================] - 2s 65ms/step - loss: 0.3159 - reconstruction_loss: 0.3701 - kl_loss: 3.9811e-06\n",
      "Epoch 178/500\n",
      "23/23 [==============================] - 2s 68ms/step - loss: 0.4007 - reconstruction_loss: 0.3419 - kl_loss: 2.0392e-06\n",
      "Epoch 179/500\n",
      "23/23 [==============================] - 2s 107ms/step - loss: 0.3150 - reconstruction_loss: 0.3097 - kl_loss: 1.3180e-06\n",
      "Epoch 180/500\n",
      "23/23 [==============================] - 2s 101ms/step - loss: 0.3901 - reconstruction_loss: 0.3228 - kl_loss: 2.7291e-06\n",
      "Epoch 181/500\n",
      "23/23 [==============================] - 1s 50ms/step - loss: 0.2491 - reconstruction_loss: 0.3467 - kl_loss: 4.1168e-06\n",
      "Epoch 182/500\n",
      "23/23 [==============================] - 2s 82ms/step - loss: 0.2901 - reconstruction_loss: 0.3848 - kl_loss: 4.9142e-06\n",
      "Epoch 183/500\n",
      "23/23 [==============================] - 2s 73ms/step - loss: 0.3656 - reconstruction_loss: 0.3242 - kl_loss: 2.3234e-06\n",
      "Epoch 184/500\n",
      "23/23 [==============================] - 2s 91ms/step - loss: 0.3706 - reconstruction_loss: 0.3491 - kl_loss: 2.8894e-06\n",
      "Epoch 185/500\n",
      "23/23 [==============================] - 3s 140ms/step - loss: 0.3358 - reconstruction_loss: 0.3814 - kl_loss: 2.3293e-06\n",
      "Epoch 186/500\n",
      "23/23 [==============================] - 1s 65ms/step - loss: 0.3902 - reconstruction_loss: 0.3390 - kl_loss: 3.1060e-06\n",
      "Epoch 187/500\n",
      "23/23 [==============================] - 2s 70ms/step - loss: 0.3155 - reconstruction_loss: 0.2874 - kl_loss: 1.9483e-06\n",
      "Epoch 188/500\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 0.3607 - reconstruction_loss: 0.3308 - kl_loss: 2.3365e-06\n",
      "Epoch 189/500\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.3992 - reconstruction_loss: 0.3622 - kl_loss: 5.1386e-06\n",
      "Epoch 190/500\n",
      "23/23 [==============================] - 1s 51ms/step - loss: 0.3178 - reconstruction_loss: 0.3490 - kl_loss: 1.2537e-06\n",
      "Epoch 191/500\n",
      "23/23 [==============================] - 2s 95ms/step - loss: 0.3556 - reconstruction_loss: 0.3349 - kl_loss: 2.5974e-06\n",
      "Epoch 192/500\n",
      "23/23 [==============================] - 3s 136ms/step - loss: 0.4747 - reconstruction_loss: 0.2918 - kl_loss: 1.6879e-06\n",
      "Epoch 193/500\n",
      "23/23 [==============================] - 2s 99ms/step - loss: 0.4499 - reconstruction_loss: 0.3640 - kl_loss: 1.5365e-06\n",
      "Epoch 194/500\n",
      "23/23 [==============================] - 1s 53ms/step - loss: 0.2789 - reconstruction_loss: 0.3374 - kl_loss: 3.2505e-06\n",
      "Epoch 195/500\n",
      "23/23 [==============================] - 2s 77ms/step - loss: 0.3792 - reconstruction_loss: 0.3644 - kl_loss: 8.1445e-07\n",
      "Epoch 196/500\n",
      "23/23 [==============================] - 1s 50ms/step - loss: 0.3029 - reconstruction_loss: 0.3158 - kl_loss: 5.5381e-06\n",
      "Epoch 197/500\n",
      "23/23 [==============================] - 3s 120ms/step - loss: 0.2883 - reconstruction_loss: 0.3494 - kl_loss: 4.2374e-06\n",
      "Epoch 198/500\n",
      "23/23 [==============================] - 2s 72ms/step - loss: 0.4266 - reconstruction_loss: 0.3192 - kl_loss: 1.6158e-06\n",
      "Epoch 199/500\n",
      "23/23 [==============================] - 2s 71ms/step - loss: 0.4111 - reconstruction_loss: 0.3612 - kl_loss: 5.5491e-06\n",
      "Epoch 200/500\n",
      "23/23 [==============================] - 1s 53ms/step - loss: 0.2980 - reconstruction_loss: 0.3829 - kl_loss: 3.3106e-06\n",
      "Epoch 201/500\n",
      "23/23 [==============================] - 1s 66ms/step - loss: 0.2173 - reconstruction_loss: 0.3673 - kl_loss: 1.4049e-06\n",
      "Epoch 202/500\n",
      "23/23 [==============================] - 1s 50ms/step - loss: 0.2489 - reconstruction_loss: 0.3668 - kl_loss: 4.6026e-06\n",
      "Epoch 203/500\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 0.3759 - reconstruction_loss: 0.3534 - kl_loss: 3.5841e-06\n",
      "Epoch 204/500\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.2911 - reconstruction_loss: 0.3601 - kl_loss: 3.3952e-06\n",
      "Epoch 205/500\n",
      "23/23 [==============================] - 2s 88ms/step - loss: 0.3273 - reconstruction_loss: 0.3888 - kl_loss: 3.1747e-06\n",
      "Epoch 206/500\n",
      "23/23 [==============================] - 3s 111ms/step - loss: 0.3138 - reconstruction_loss: 0.3701 - kl_loss: 2.9851e-06\n",
      "Epoch 207/500\n",
      "23/23 [==============================] - 2s 92ms/step - loss: 0.3379 - reconstruction_loss: 0.3433 - kl_loss: 1.8220e-06\n",
      "Epoch 208/500\n",
      "23/23 [==============================] - 2s 67ms/step - loss: 0.2839 - reconstruction_loss: 0.3238 - kl_loss: 5.5687e-06\n",
      "Epoch 209/500\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 0.2793 - reconstruction_loss: 0.3402 - kl_loss: 3.8674e-06\n",
      "Epoch 210/500\n",
      "23/23 [==============================] - 2s 68ms/step - loss: 0.3479 - reconstruction_loss: 0.3413 - kl_loss: 5.7992e-06\n",
      "Epoch 211/500\n",
      "23/23 [==============================] - 2s 65ms/step - loss: 0.4452 - reconstruction_loss: 0.3331 - kl_loss: 3.0445e-06\n",
      "Epoch 212/500\n",
      "23/23 [==============================] - 2s 67ms/step - loss: 0.3441 - reconstruction_loss: 0.3734 - kl_loss: 3.0420e-06\n",
      "Epoch 213/500\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.2809 - reconstruction_loss: 0.3461 - kl_loss: 4.4868e-06\n",
      "Epoch 214/500\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 0.3483 - reconstruction_loss: 0.3450 - kl_loss: 7.4955e-06\n",
      "Epoch 215/500\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 0.3218 - reconstruction_loss: 0.3355 - kl_loss: 3.4897e-06\n",
      "Epoch 216/500\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 0.3715 - reconstruction_loss: 0.3788 - kl_loss: 8.0186e-06\n",
      "Epoch 217/500\n",
      "23/23 [==============================] - 2s 67ms/step - loss: 0.4313 - reconstruction_loss: 0.3775 - kl_loss: 4.7291e-06\n",
      "Epoch 218/500\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 0.4122 - reconstruction_loss: 0.3554 - kl_loss: 1.7774e-06\n",
      "Epoch 219/500\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 0.2685 - reconstruction_loss: 0.3312 - kl_loss: 1.4655e-06\n",
      "Epoch 220/500\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 0.3323 - reconstruction_loss: 0.3144 - kl_loss: 6.5801e-06\n",
      "Epoch 221/500\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 0.4147 - reconstruction_loss: 0.3765 - kl_loss: 3.3164e-06\n",
      "Epoch 222/500\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.4403 - reconstruction_loss: 0.3717 - kl_loss: 5.4834e-06\n",
      "Epoch 223/500\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 0.4046 - reconstruction_loss: 0.3583 - kl_loss: 5.8680e-06\n",
      "Epoch 224/500\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 0.3201 - reconstruction_loss: 0.3318 - kl_loss: 2.5666e-06\n",
      "Epoch 225/500\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 0.4759 - reconstruction_loss: 0.3367 - kl_loss: 1.0013e-05\n",
      "Epoch 226/500\n",
      "23/23 [==============================] - 2s 68ms/step - loss: 0.5147 - reconstruction_loss: 0.3142 - kl_loss: 8.1985e-06\n",
      "Epoch 227/500\n",
      "23/23 [==============================] - 1s 63ms/step - loss: 0.3552 - reconstruction_loss: 0.3575 - kl_loss: 1.2938e-05\n",
      "Epoch 228/500\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 0.3147 - reconstruction_loss: 0.3454 - kl_loss: 6.9099e-06\n",
      "Epoch 229/500\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 0.3543 - reconstruction_loss: 0.3213 - kl_loss: 9.1880e-06\n",
      "Epoch 230/500\n",
      "23/23 [==============================] - 2s 68ms/step - loss: 0.3603 - reconstruction_loss: 0.3749 - kl_loss: 6.7367e-06\n",
      "Epoch 231/500\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.4097 - reconstruction_loss: 0.3473 - kl_loss: 5.2406e-06\n",
      "Epoch 232/500\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 0.3522 - reconstruction_loss: 0.3539 - kl_loss: 6.6312e-06\n",
      "Epoch 233/500\n",
      "23/23 [==============================] - 2s 71ms/step - loss: 0.1896 - reconstruction_loss: 0.3544 - kl_loss: 1.1680e-05\n",
      "Epoch 234/500\n",
      "23/23 [==============================] - 2s 73ms/step - loss: 0.3735 - reconstruction_loss: 0.3676 - kl_loss: 7.8787e-06\n",
      "Epoch 235/500\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 0.3921 - reconstruction_loss: 0.3681 - kl_loss: 6.0861e-06\n",
      "Epoch 236/500\n",
      "23/23 [==============================] - 2s 69ms/step - loss: 0.3223 - reconstruction_loss: 0.3386 - kl_loss: 3.3102e-06\n",
      "Epoch 237/500\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 0.3506 - reconstruction_loss: 0.3364 - kl_loss: 3.1909e-06\n",
      "Epoch 238/500\n",
      "23/23 [==============================] - 1s 64ms/step - loss: 0.2375 - reconstruction_loss: 0.3529 - kl_loss: 4.0362e-06\n",
      "Epoch 239/500\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 0.3049 - reconstruction_loss: 0.3672 - kl_loss: 5.0013e-06\n",
      "Epoch 240/500\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 0.3902 - reconstruction_loss: 0.3305 - kl_loss: 8.5427e-06\n",
      "Epoch 241/500\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 0.3119 - reconstruction_loss: 0.3157 - kl_loss: 1.8966e-05\n",
      "Epoch 242/500\n",
      "23/23 [==============================] - 1s 65ms/step - loss: 0.3035 - reconstruction_loss: 0.3855 - kl_loss: 8.9749e-06\n",
      "Epoch 243/500\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 0.2313 - reconstruction_loss: 0.3397 - kl_loss: 4.1595e-06\n",
      "Epoch 244/500\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 0.2832 - reconstruction_loss: 0.3741 - kl_loss: 8.1505e-06\n",
      "Epoch 245/500\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 0.2534 - reconstruction_loss: 0.3670 - kl_loss: 1.4853e-05\n",
      "Epoch 246/500\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.3456 - reconstruction_loss: 0.3240 - kl_loss: 2.8808e-05\n",
      "Epoch 247/500\n",
      "23/23 [==============================] - 1s 65ms/step - loss: 0.2971 - reconstruction_loss: 0.3159 - kl_loss: 6.7498e-06\n",
      "Epoch 248/500\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 0.3327 - reconstruction_loss: 0.3705 - kl_loss: 1.2239e-05\n",
      "Epoch 249/500\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 0.3865 - reconstruction_loss: 0.3278 - kl_loss: 4.8330e-06\n",
      "Epoch 250/500\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.3840 - reconstruction_loss: 0.3431 - kl_loss: 8.5458e-06\n",
      "Epoch 251/500\n",
      "23/23 [==============================] - 2s 65ms/step - loss: 0.4093 - reconstruction_loss: 0.4069 - kl_loss: 9.0884e-06\n",
      "Epoch 252/500\n",
      "23/23 [==============================] - 1s 63ms/step - loss: 0.3525 - reconstruction_loss: 0.3867 - kl_loss: 8.5882e-06\n",
      "Epoch 253/500\n",
      "23/23 [==============================] - 2s 67ms/step - loss: 0.3475 - reconstruction_loss: 0.3715 - kl_loss: 8.2490e-06\n",
      "Epoch 254/500\n",
      "23/23 [==============================] - 2s 68ms/step - loss: 0.3475 - reconstruction_loss: 0.3470 - kl_loss: 9.0283e-06\n",
      "Epoch 255/500\n",
      "23/23 [==============================] - 1s 63ms/step - loss: 0.3727 - reconstruction_loss: 0.3158 - kl_loss: 1.2789e-05\n",
      "Epoch 256/500\n",
      "23/23 [==============================] - 2s 68ms/step - loss: 0.3779 - reconstruction_loss: 0.3687 - kl_loss: 8.0342e-06\n",
      "Epoch 257/500\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 0.2387 - reconstruction_loss: 0.3187 - kl_loss: 6.2886e-06\n",
      "Epoch 258/500\n",
      "23/23 [==============================] - 2s 67ms/step - loss: 0.3090 - reconstruction_loss: 0.3349 - kl_loss: 1.9653e-05\n",
      "Epoch 259/500\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 0.4373 - reconstruction_loss: 0.3801 - kl_loss: 2.9118e-05\n",
      "Epoch 260/500\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 0.4424 - reconstruction_loss: 0.3585 - kl_loss: 1.1278e-05\n",
      "Epoch 261/500\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 0.2299 - reconstruction_loss: 0.3710 - kl_loss: 1.0124e-05\n",
      "Epoch 262/500\n",
      "23/23 [==============================] - 2s 74ms/step - loss: 0.3697 - reconstruction_loss: 0.3132 - kl_loss: 3.5959e-05\n",
      "Epoch 263/500\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 0.3705 - reconstruction_loss: 0.3380 - kl_loss: 1.2588e-05\n",
      "Epoch 264/500\n",
      "23/23 [==============================] - 2s 82ms/step - loss: 0.2764 - reconstruction_loss: 0.3760 - kl_loss: 1.5729e-05\n",
      "Epoch 265/500\n",
      "23/23 [==============================] - 2s 66ms/step - loss: 0.3466 - reconstruction_loss: 0.3620 - kl_loss: 2.7842e-05\n",
      "Epoch 266/500\n",
      "23/23 [==============================] - 2s 80ms/step - loss: 0.3730 - reconstruction_loss: 0.3304 - kl_loss: 3.8186e-05\n",
      "Epoch 267/500\n",
      "23/23 [==============================] - 1s 64ms/step - loss: 0.2207 - reconstruction_loss: 0.3676 - kl_loss: 4.4832e-05\n",
      "Epoch 268/500\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 0.4021 - reconstruction_loss: 0.3607 - kl_loss: 1.4408e-04\n",
      "Epoch 269/500\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 0.3682 - reconstruction_loss: 0.3529 - kl_loss: 7.0465e-05\n",
      "Epoch 270/500\n",
      "23/23 [==============================] - 2s 68ms/step - loss: 0.4043 - reconstruction_loss: 0.3468 - kl_loss: 5.9020e-05\n",
      "Epoch 271/500\n",
      "23/23 [==============================] - 2s 91ms/step - loss: 0.2900 - reconstruction_loss: 0.3627 - kl_loss: 1.2095e-04\n",
      "Epoch 272/500\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 0.4102 - reconstruction_loss: 0.3211 - kl_loss: 1.3791e-04\n",
      "Epoch 273/500\n",
      "23/23 [==============================] - 1s 60ms/step - loss: 0.3810 - reconstruction_loss: 0.3474 - kl_loss: 2.3062e-04\n",
      "Epoch 274/500\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 0.3417 - reconstruction_loss: 0.3327 - kl_loss: 8.8487e-05\n",
      "Epoch 275/500\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 0.3642 - reconstruction_loss: 0.3568 - kl_loss: 7.6929e-05\n",
      "Epoch 276/500\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 0.4584 - reconstruction_loss: 0.2904 - kl_loss: 1.7928e-04\n",
      "Epoch 277/500\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 0.3962 - reconstruction_loss: 0.3151 - kl_loss: 2.8442e-04\n",
      "Epoch 278/500\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 0.3533 - reconstruction_loss: 0.3525 - kl_loss: 5.0647e-04\n",
      "Epoch 279/500\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 0.3657 - reconstruction_loss: 0.3321 - kl_loss: 5.6605e-04\n",
      "Epoch 280/500\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 0.2723 - reconstruction_loss: 0.3962 - kl_loss: 6.6228e-04\n",
      "Epoch 281/500\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 0.3070 - reconstruction_loss: 0.3428 - kl_loss: 6.4765e-04\n",
      "Epoch 282/500\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 0.2817 - reconstruction_loss: 0.3331 - kl_loss: 8.8509e-04\n",
      "Epoch 283/500\n",
      "23/23 [==============================] - 1s 57ms/step - loss: 0.2128 - reconstruction_loss: 0.3467 - kl_loss: 0.0012\n",
      "Epoch 284/500\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 0.3569 - reconstruction_loss: 0.3715 - kl_loss: 0.0018\n",
      "Epoch 285/500\n",
      "23/23 [==============================] - 1s 64ms/step - loss: 0.2589 - reconstruction_loss: 0.3262 - kl_loss: 0.0012\n",
      "Epoch 286/500\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 0.3047 - reconstruction_loss: 0.3418 - kl_loss: 9.6840e-05\n",
      "Epoch 287/500\n",
      "23/23 [==============================] - 1s 58ms/step - loss: 0.2883 - reconstruction_loss: 0.3846 - kl_loss: 5.7298e-04\n",
      "Epoch 288/500\n",
      "23/23 [==============================] - 2s 70ms/step - loss: 0.3839 - reconstruction_loss: 0.3030 - kl_loss: 4.8283e-04\n",
      "Epoch 289/500\n",
      "23/23 [==============================] - 1s 61ms/step - loss: 0.3996 - reconstruction_loss: 0.3610 - kl_loss: 8.6366e-05\n",
      "Epoch 290/500\n",
      "23/23 [==============================] - 2s 68ms/step - loss: 0.4013 - reconstruction_loss: 0.3200 - kl_loss: 1.3873e-04\n",
      "Epoch 291/500\n",
      "23/23 [==============================] - 1s 63ms/step - loss: 0.3234 - reconstruction_loss: 0.3514 - kl_loss: 3.4374e-04\n",
      "Epoch 292/500\n",
      "23/23 [==============================] - 2s 99ms/step - loss: 0.2397 - reconstruction_loss: 0.3503 - kl_loss: 7.1752e-04\n",
      "Epoch 293/500\n",
      "23/23 [==============================] - 2s 79ms/step - loss: 0.2232 - reconstruction_loss: 0.3859 - kl_loss: 0.0011\n",
      "Epoch 294/500\n",
      "23/23 [==============================] - 2s 104ms/step - loss: 0.4229 - reconstruction_loss: 0.3284 - kl_loss: 0.0013\n",
      "Epoch 295/500\n",
      "23/23 [==============================] - 1s 59ms/step - loss: 0.4038 - reconstruction_loss: 0.3652 - kl_loss: 0.0017\n",
      "Epoch 296/500\n",
      "23/23 [==============================] - 2s 87ms/step - loss: 0.2075 - reconstruction_loss: 0.3653 - kl_loss: 0.0026\n",
      "Epoch 297/500\n",
      "23/23 [==============================] - 2s 103ms/step - loss: 0.4283 - reconstruction_loss: 0.3413 - kl_loss: 0.0024\n",
      "Epoch 298/500\n",
      "23/23 [==============================] - 1s 55ms/step - loss: 0.2757 - reconstruction_loss: 0.3308 - kl_loss: 0.0032\n",
      "Epoch 299/500\n",
      "23/23 [==============================] - 2s 79ms/step - loss: 0.4884 - reconstruction_loss: 0.3418 - kl_loss: 0.0037\n",
      "Epoch 300/500\n",
      "23/23 [==============================] - 1s 49ms/step - loss: 0.3672 - reconstruction_loss: 0.3809 - kl_loss: 0.0047\n",
      "Epoch 301/500\n",
      "23/23 [==============================] - 1s 56ms/step - loss: 0.3696 - reconstruction_loss: 0.3448 - kl_loss: 0.0053\n",
      "Epoch 302/500\n",
      "23/23 [==============================] - 1s 54ms/step - loss: 0.3994 - reconstruction_loss: 0.3621 - kl_loss: 0.0065\n",
      "Epoch 303/500\n",
      "23/23 [==============================] - 1s 44ms/step - loss: 0.3623 - reconstruction_loss: 0.3335 - kl_loss: 0.0070\n",
      "Epoch 304/500\n",
      "23/23 [==============================] - 1s 62ms/step - loss: 0.4292 - reconstruction_loss: 0.2801 - kl_loss: 0.0093\n",
      "Epoch 305/500\n",
      "23/23 [==============================] - 1s 52ms/step - loss: 0.3545 - reconstruction_loss: 0.3428 - kl_loss: 0.0146\n",
      "Epoch 306/500\n",
      "23/23 [==============================] - 1s 52ms/step - loss: 0.3272 - reconstruction_loss: 0.3544 - kl_loss: 0.0166\n",
      "Epoch 307/500\n",
      "23/23 [==============================] - 1s 46ms/step - loss: 0.3545 - reconstruction_loss: 0.3413 - kl_loss: 0.0209\n",
      "Epoch 308/500\n",
      "23/23 [==============================] - 1s 44ms/step - loss: 0.3079 - reconstruction_loss: 0.2967 - kl_loss: 0.0248\n",
      "Epoch 309/500\n",
      "23/23 [==============================] - 1s 45ms/step - loss: 0.2332 - reconstruction_loss: 0.2532 - kl_loss: 0.0463\n",
      "Epoch 310/500\n",
      "23/23 [==============================] - 1s 46ms/step - loss: 0.2827 - reconstruction_loss: 0.2306 - kl_loss: 0.0788\n",
      "Epoch 311/500\n",
      "23/23 [==============================] - 1s 48ms/step - loss: 0.1537 - reconstruction_loss: -0.0326 - kl_loss: 0.2407\n",
      "Epoch 312/500\n",
      "23/23 [==============================] - 1s 52ms/step - loss: -1.6298 - reconstruction_loss: -9.5431 - kl_loss: 2.0232\n",
      "Epoch 313/500\n",
      "23/23 [==============================] - 2s 75ms/step - loss: -451.2322 - reconstruction_loss: -1299.2144 - kl_loss: 60.7727\n",
      "Epoch 314/500\n",
      "23/23 [==============================] - 2s 94ms/step - loss: -25853.0723 - reconstruction_loss: -52126.5234 - kl_loss: 1698.6744\n",
      "Epoch 315/500\n",
      "23/23 [==============================] - 3s 120ms/step - loss: -409721.0410 - reconstruction_loss: -626453.9375 - kl_loss: 9493.8076\n",
      "Epoch 316/500\n",
      "23/23 [==============================] - 2s 89ms/step - loss: -3220680.3646 - reconstruction_loss: -4999568.0000 - kl_loss: 42814.9492\n",
      "Epoch 317/500\n",
      "23/23 [==============================] - 2s 96ms/step - loss: -18686089.9167 - reconstruction_loss: -25169488.0000 - kl_loss: 158094.5938\n",
      "Epoch 318/500\n",
      "23/23 [==============================] - 2s 79ms/step - loss: -83051471.0000 - reconstruction_loss: -91361800.0000 - kl_loss: 14662201.0000\n",
      "Epoch 319/500\n",
      "23/23 [==============================] - 2s 83ms/step - loss: -115278732.8333 - reconstruction_loss: -122774392.0000 - kl_loss: 467978.2188\n",
      "Epoch 320/500\n",
      "23/23 [==============================] - 2s 69ms/step - loss: -222218014.0000 - reconstruction_loss: -226474784.0000 - kl_loss: 747577.9375\n",
      "Epoch 321/500\n",
      "23/23 [==============================] - 2s 70ms/step - loss: 1471530049.3333 - reconstruction_loss: 5415579648.0000 - kl_loss: 9158297600.0000\n",
      "Epoch 322/500\n",
      "23/23 [==============================] - 3s 132ms/step - loss: -439444794.6667 - reconstruction_loss: -458935776.0000 - kl_loss: 5854907.0000\n",
      "Epoch 323/500\n",
      "23/23 [==============================] - 1s 50ms/step - loss: -372745449.3333 - reconstruction_loss: -412508064.0000 - kl_loss: 8468359.0000\n",
      "Epoch 324/500\n",
      "23/23 [==============================] - 1s 50ms/step - loss: -236328062.6667 - reconstruction_loss: -401623936.0000 - kl_loss: 8939189.0000\n",
      "Epoch 325/500\n",
      "23/23 [==============================] - 1s 52ms/step - loss: -394365901.3333 - reconstruction_loss: -503006112.0000 - kl_loss: 9349646.0000\n",
      "Epoch 326/500\n",
      "23/23 [==============================] - 1s 59ms/step - loss: -388469064.0833 - reconstruction_loss: -455425440.0000 - kl_loss: 9638201.0000\n",
      "Epoch 327/500\n",
      "23/23 [==============================] - 1s 60ms/step - loss: -546313921.3333 - reconstruction_loss: -504528000.0000 - kl_loss: 10018392.0000\n",
      "Epoch 328/500\n",
      "23/23 [==============================] - 1s 59ms/step - loss: -473537914.6667 - reconstruction_loss: -548741312.0000 - kl_loss: 10207548.0000\n",
      "Epoch 329/500\n",
      "23/23 [==============================] - 1s 47ms/step - loss: -602712178.6667 - reconstruction_loss: -575356928.0000 - kl_loss: 10492815.0000\n",
      "Epoch 330/500\n",
      "23/23 [==============================] - 1s 61ms/step - loss: -565011817.3333 - reconstruction_loss: -674990016.0000 - kl_loss: 10776868.0000\n",
      "Epoch 331/500\n",
      "23/23 [==============================] - 2s 67ms/step - loss: -669234928.0000 - reconstruction_loss: -606349568.0000 - kl_loss: 11063576.0000\n",
      "Epoch 332/500\n",
      "23/23 [==============================] - 1s 46ms/step - loss: -675065166.6667 - reconstruction_loss: -597652736.0000 - kl_loss: 11268690.0000\n",
      "Epoch 333/500\n",
      "23/23 [==============================] - 1s 47ms/step - loss: -817478434.6667 - reconstruction_loss: -675782016.0000 - kl_loss: 11524851.0000\n",
      "Epoch 334/500\n",
      "23/23 [==============================] - 1s 49ms/step - loss: -692891473.3333 - reconstruction_loss: -752650880.0000 - kl_loss: 11720430.0000\n",
      "Epoch 335/500\n",
      "23/23 [==============================] - 1s 51ms/step - loss: -889693661.3333 - reconstruction_loss: -674386816.0000 - kl_loss: 11900534.0000\n",
      "Epoch 336/500\n",
      "23/23 [==============================] - 1s 54ms/step - loss: -597485206.6667 - reconstruction_loss: -816009024.0000 - kl_loss: 12123164.0000\n",
      "Epoch 337/500\n",
      "23/23 [==============================] - 1s 62ms/step - loss: -767652732.0000 - reconstruction_loss: -821334720.0000 - kl_loss: 12397825.0000\n",
      "Epoch 338/500\n",
      "23/23 [==============================] - 1s 52ms/step - loss: -713225510.6667 - reconstruction_loss: -851806720.0000 - kl_loss: 12613633.0000\n",
      "Epoch 339/500\n",
      "23/23 [==============================] - 1s 53ms/step - loss: -831262426.6667 - reconstruction_loss: -868779328.0000 - kl_loss: 12816366.0000\n",
      "Epoch 340/500\n",
      "23/23 [==============================] - 2s 85ms/step - loss: -778027516.6667 - reconstruction_loss: -894252992.0000 - kl_loss: 13052391.0000\n",
      "Epoch 341/500\n",
      "23/23 [==============================] - 1s 55ms/step - loss: -785850080.0000 - reconstruction_loss: -989528768.0000 - kl_loss: 13307388.0000\n",
      "Epoch 342/500\n",
      "23/23 [==============================] - 1s 59ms/step - loss: -1015714698.6667 - reconstruction_loss: -953330624.0000 - kl_loss: 13514656.0000\n",
      "Epoch 343/500\n",
      "23/23 [==============================] - 1s 45ms/step - loss: -1000667352.0000 - reconstruction_loss: -1147919616.0000 - kl_loss: 13888730.0000\n",
      "Epoch 344/500\n",
      "23/23 [==============================] - 1s 47ms/step - loss: -1395276834.6667 - reconstruction_loss: -1084465792.0000 - kl_loss: 14079167.0000\n",
      "Epoch 345/500\n",
      "23/23 [==============================] - 1s 56ms/step - loss: -987955805.3333 - reconstruction_loss: -1144477184.0000 - kl_loss: 14292099.0000\n",
      "Epoch 346/500\n",
      "23/23 [==============================] - 1s 51ms/step - loss: -1080648629.3333 - reconstruction_loss: -1165346304.0000 - kl_loss: 14499386.0000\n",
      "Epoch 347/500\n",
      "23/23 [==============================] - 1s 54ms/step - loss: -1055122098.6667 - reconstruction_loss: -1174243200.0000 - kl_loss: 14675809.0000\n",
      "Epoch 348/500\n",
      "23/23 [==============================] - 1s 55ms/step - loss: -1497040229.3333 - reconstruction_loss: -1239680128.0000 - kl_loss: 14922397.0000\n",
      "Epoch 349/500\n",
      "23/23 [==============================] - 1s 43ms/step - loss: -1203812944.0000 - reconstruction_loss: -1366119680.0000 - kl_loss: 15121828.0000\n",
      "Epoch 350/500\n",
      "23/23 [==============================] - 1s 46ms/step - loss: -1506972122.6667 - reconstruction_loss: -1404345088.0000 - kl_loss: 15366038.0000\n",
      "Epoch 351/500\n",
      "23/23 [==============================] - 1s 57ms/step - loss: -1334587102.0000 - reconstruction_loss: -1419761536.0000 - kl_loss: 15524686.0000\n",
      "Epoch 352/500\n",
      "23/23 [==============================] - 1s 50ms/step - loss: -1497475383.3333 - reconstruction_loss: -1516385664.0000 - kl_loss: 15737575.0000\n",
      "Epoch 353/500\n",
      "23/23 [==============================] - 1s 55ms/step - loss: -1464536845.6667 - reconstruction_loss: -1524752512.0000 - kl_loss: 15851311.0000\n",
      "Epoch 354/500\n",
      "23/23 [==============================] - 1s 49ms/step - loss: -2098500048.0000 - reconstruction_loss: -1672687104.0000 - kl_loss: 16158675.0000\n",
      "Epoch 355/500\n",
      "23/23 [==============================] - 1s 45ms/step - loss: -2095926592.0000 - reconstruction_loss: -1700731520.0000 - kl_loss: 16327407.0000\n",
      "Epoch 356/500\n",
      "23/23 [==============================] - 1s 54ms/step - loss: -1341154832.0000 - reconstruction_loss: -2010396288.0000 - kl_loss: 16500910.0000\n",
      "Epoch 357/500\n",
      "23/23 [==============================] - 1s 51ms/step - loss: -2454738549.3333 - reconstruction_loss: -1794981760.0000 - kl_loss: 16653149.0000\n",
      "Epoch 358/500\n",
      "23/23 [==============================] - 1s 44ms/step - loss: -2080040986.6667 - reconstruction_loss: -1997995264.0000 - kl_loss: 16878738.0000\n",
      "Epoch 359/500\n",
      "23/23 [==============================] - 1s 44ms/step - loss: -1795022816.0000 - reconstruction_loss: -2153021952.0000 - kl_loss: 16946232.0000\n",
      "Epoch 360/500\n",
      "23/23 [==============================] - 1s 51ms/step - loss: -1720920920.8333 - reconstruction_loss: -2276872704.0000 - kl_loss: 17081870.0000\n",
      "Epoch 361/500\n",
      "23/23 [==============================] - 1s 42ms/step - loss: -2180476808.3333 - reconstruction_loss: -2434623232.0000 - kl_loss: 17343142.0000\n",
      "Epoch 362/500\n",
      "23/23 [==============================] - 1s 52ms/step - loss: -2149259178.6667 - reconstruction_loss: -2516105728.0000 - kl_loss: 17460356.0000\n",
      "Epoch 363/500\n",
      "23/23 [==============================] - 1s 63ms/step - loss: -2676337216.0000 - reconstruction_loss: -2823083520.0000 - kl_loss: 17723082.0000\n",
      "Epoch 364/500\n",
      "23/23 [==============================] - 3s 107ms/step - loss: -3271916842.6667 - reconstruction_loss: -2895300352.0000 - kl_loss: 17898004.0000\n",
      "Epoch 365/500\n",
      "23/23 [==============================] - 1s 43ms/step - loss: -3921488032.0000 - reconstruction_loss: -3186754304.0000 - kl_loss: 18200318.0000\n",
      "Epoch 366/500\n",
      "23/23 [==============================] - 1s 42ms/step - loss: -2749610046.6667 - reconstruction_loss: -3160292352.0000 - kl_loss: 18153504.0000\n",
      "Epoch 367/500\n",
      "23/23 [==============================] - 1s 42ms/step - loss: -3297990517.3333 - reconstruction_loss: -3557489920.0000 - kl_loss: 18369296.0000\n",
      "Epoch 368/500\n",
      "23/23 [==============================] - 2s 76ms/step - loss: -3523666485.3333 - reconstruction_loss: -4364641792.0000 - kl_loss: 18636078.0000\n",
      "Epoch 369/500\n",
      "23/23 [==============================] - 1s 47ms/step - loss: -3850514784.0000 - reconstruction_loss: -4025009664.0000 - kl_loss: 18652716.0000\n",
      "Epoch 370/500\n",
      "23/23 [==============================] - 1s 41ms/step - loss: -4229196480.0000 - reconstruction_loss: -4788072960.0000 - kl_loss: 19005328.0000\n",
      "Epoch 371/500\n",
      "23/23 [==============================] - 1s 41ms/step - loss: -4994506464.0000 - reconstruction_loss: -4979217408.0000 - kl_loss: 19103184.0000\n",
      "Epoch 372/500\n",
      "23/23 [==============================] - 1s 43ms/step - loss: -4678245461.3333 - reconstruction_loss: -5016220672.0000 - kl_loss: 19129280.0000\n",
      "Epoch 373/500\n",
      "23/23 [==============================] - 1s 41ms/step - loss: -5386763861.3333 - reconstruction_loss: -5907364352.0000 - kl_loss: 19249976.0000\n",
      "Epoch 374/500\n",
      "23/23 [==============================] - 1s 41ms/step - loss: -5181121568.0000 - reconstruction_loss: -5946807808.0000 - kl_loss: 19161862.0000\n",
      "Epoch 375/500\n",
      "23/23 [==============================] - 1s 41ms/step - loss: -5919041536.0000 - reconstruction_loss: -6721766912.0000 - kl_loss: 19227208.0000\n",
      "Epoch 376/500\n",
      "23/23 [==============================] - 1s 40ms/step - loss: -8260692266.6667 - reconstruction_loss: -7022326784.0000 - kl_loss: 19235200.0000\n",
      "Epoch 377/500\n",
      "23/23 [==============================] - 1s 52ms/step - loss: -6673015829.3333 - reconstruction_loss: -7602371072.0000 - kl_loss: 19044454.0000\n",
      "Epoch 378/500\n",
      "23/23 [==============================] - 2s 71ms/step - loss: -8002253130.6667 - reconstruction_loss: -8306362368.0000 - kl_loss: 18874712.0000\n",
      "Epoch 379/500\n",
      "23/23 [==============================] - 2s 108ms/step - loss: -9006184544.0000 - reconstruction_loss: -9330045952.0000 - kl_loss: 18680102.0000\n",
      "Epoch 380/500\n",
      "23/23 [==============================] - 3s 114ms/step - loss: -9925895124.0000 - reconstruction_loss: -10158128128.0000 - kl_loss: 18402284.0000\n",
      "Epoch 381/500\n",
      "23/23 [==============================] - 3s 113ms/step - loss: -11360806784.0000 - reconstruction_loss: -11086283776.0000 - kl_loss: 18065148.0000\n",
      "Epoch 382/500\n",
      "23/23 [==============================] - 2s 89ms/step - loss: -10332229376.0000 - reconstruction_loss: -12855346176.0000 - kl_loss: 17802440.0000\n",
      "Epoch 383/500\n",
      "23/23 [==============================] - 1s 56ms/step - loss: -12726478677.3333 - reconstruction_loss: -14351614976.0000 - kl_loss: 17495492.0000\n",
      "Epoch 384/500\n",
      "23/23 [==============================] - 1s 55ms/step - loss: -18825448320.0000 - reconstruction_loss: -15181757440.0000 - kl_loss: 17090344.0000\n",
      "Epoch 385/500\n",
      "23/23 [==============================] - 1s 60ms/step - loss: -16765397632.0000 - reconstruction_loss: -16924844032.0000 - kl_loss: 16332419.0000\n",
      "Epoch 386/500\n",
      "23/23 [==============================] - 1s 60ms/step - loss: -17671465514.6667 - reconstruction_loss: -18922127360.0000 - kl_loss: 15448022.0000\n",
      "Epoch 387/500\n",
      "23/23 [==============================] - 2s 67ms/step - loss: -19923119829.3333 - reconstruction_loss: -22168659968.0000 - kl_loss: 14716881.0000\n",
      "Epoch 388/500\n",
      "23/23 [==============================] - 2s 73ms/step - loss: -22756760704.0000 - reconstruction_loss: -23488899072.0000 - kl_loss: 13855190.0000\n",
      "Epoch 389/500\n",
      "23/23 [==============================] - 1s 43ms/step - loss: -24209880832.0000 - reconstruction_loss: -26894796800.0000 - kl_loss: 12989992.0000\n",
      "Epoch 390/500\n",
      "23/23 [==============================] - 1s 46ms/step - loss: -28776664832.0000 - reconstruction_loss: -29813995520.0000 - kl_loss: 12353663.0000\n",
      "Epoch 391/500\n",
      "23/23 [==============================] - 1s 47ms/step - loss: -30937439061.3333 - reconstruction_loss: -33977815040.0000 - kl_loss: 11934517.0000\n",
      "Epoch 392/500\n",
      "23/23 [==============================] - 1s 50ms/step - loss: -34807166037.3333 - reconstruction_loss: -39416348672.0000 - kl_loss: 11898318.0000\n",
      "Epoch 393/500\n",
      "23/23 [==============================] - 2s 82ms/step - loss: -41014537045.3333 - reconstruction_loss: -43368050688.0000 - kl_loss: 11851334.0000\n",
      "Epoch 394/500\n",
      "23/23 [==============================] - 2s 82ms/step - loss: -45167745194.6667 - reconstruction_loss: -49031991296.0000 - kl_loss: 12341987.0000\n",
      "Epoch 395/500\n",
      "23/23 [==============================] - 2s 70ms/step - loss: -56209665194.6667 - reconstruction_loss: -54508843008.0000 - kl_loss: 13070029.0000\n",
      "Epoch 396/500\n",
      "23/23 [==============================] - 2s 76ms/step - loss: -56430842026.6667 - reconstruction_loss: -64285364224.0000 - kl_loss: 13696925.0000\n",
      "Epoch 397/500\n",
      "23/23 [==============================] - 2s 89ms/step - loss: -70955756544.0000 - reconstruction_loss: -73118203904.0000 - kl_loss: 15162408.0000\n",
      "Epoch 398/500\n",
      "23/23 [==============================] - 2s 72ms/step - loss: -75300342272.0000 - reconstruction_loss: -79872098304.0000 - kl_loss: 16150496.0000\n",
      "Epoch 399/500\n",
      "23/23 [==============================] - 2s 73ms/step - loss: -80847229440.0000 - reconstruction_loss: -88825569280.0000 - kl_loss: 17614210.0000\n",
      "Epoch 400/500\n",
      "23/23 [==============================] - 2s 71ms/step - loss: -95719428778.6667 - reconstruction_loss: -113573011456.0000 - kl_loss: 19670378.0000\n",
      "Epoch 401/500\n",
      "23/23 [==============================] - 2s 96ms/step - loss: -104514207061.3333 - reconstruction_loss: -123639250944.0000 - kl_loss: 22238650.0000\n",
      "Epoch 402/500\n",
      "23/23 [==============================] - 2s 85ms/step - loss: -126817298773.3333 - reconstruction_loss: -131146440704.0000 - kl_loss: 25062686.0000\n",
      "Epoch 403/500\n",
      "23/23 [==============================] - 2s 77ms/step - loss: -140669374122.6667 - reconstruction_loss: -152694767616.0000 - kl_loss: 27577636.0000\n",
      "Epoch 404/500\n",
      "23/23 [==============================] - 2s 80ms/step - loss: -160304127317.3333 - reconstruction_loss: -182992781312.0000 - kl_loss: 31101476.0000\n",
      "Epoch 405/500\n",
      "23/23 [==============================] - 2s 73ms/step - loss: -198410623658.6667 - reconstruction_loss: -197129961472.0000 - kl_loss: 34684892.0000\n",
      "Epoch 406/500\n",
      "23/23 [==============================] - 1s 61ms/step - loss: -217992214528.0000 - reconstruction_loss: -228788912128.0000 - kl_loss: 39122940.0000\n",
      "Epoch 407/500\n",
      "23/23 [==============================] - 1s 65ms/step - loss: -241602782549.3333 - reconstruction_loss: -258136358912.0000 - kl_loss: 44934324.0000\n",
      "Epoch 408/500\n",
      "23/23 [==============================] - 1s 64ms/step - loss: -275380092245.3333 - reconstruction_loss: -292380409856.0000 - kl_loss: 48633704.0000\n",
      "Epoch 409/500\n",
      "23/23 [==============================] - 2s 83ms/step - loss: -286924920149.3333 - reconstruction_loss: -334702477312.0000 - kl_loss: 53985012.0000\n",
      "Epoch 410/500\n",
      "23/23 [==============================] - 1s 59ms/step - loss: -360519411029.3333 - reconstruction_loss: -384617054208.0000 - kl_loss: 57444508.0000\n",
      "Epoch 411/500\n",
      "23/23 [==============================] - 1s 59ms/step - loss: -432214168917.3333 - reconstruction_loss: -441429295104.0000 - kl_loss: 68425776.0000\n",
      "Epoch 412/500\n",
      "23/23 [==============================] - 3s 115ms/step - loss: -464488724480.0000 - reconstruction_loss: -514980511744.0000 - kl_loss: 78360600.0000\n",
      "Epoch 413/500\n",
      "23/23 [==============================] - 1s 62ms/step - loss: -594224260437.3334 - reconstruction_loss: -615176077312.0000 - kl_loss: 97753088.0000\n",
      "Epoch 414/500\n",
      "23/23 [==============================] - 1s 63ms/step - loss: -758297143978.6666 - reconstruction_loss: -717592199168.0000 - kl_loss: 113299568.0000\n",
      "Epoch 415/500\n",
      "23/23 [==============================] - 3s 137ms/step - loss: -774735025493.3334 - reconstruction_loss: -892966404096.0000 - kl_loss: 144269216.0000\n",
      "Epoch 416/500\n",
      "23/23 [==============================] - 2s 64ms/step - loss: -965622726656.0000 - reconstruction_loss: -1002412965888.0000 - kl_loss: 172757216.0000\n",
      "Epoch 417/500\n",
      "23/23 [==============================] - 2s 93ms/step - loss: -1083316811093.3334 - reconstruction_loss: -1261990707200.0000 - kl_loss: 218822832.0000\n",
      "Epoch 418/500\n",
      "23/23 [==============================] - 2s 83ms/step - loss: -1255089116501.3333 - reconstruction_loss: -1405234184192.0000 - kl_loss: 261154880.0000\n",
      "Epoch 419/500\n",
      "23/23 [==============================] - 2s 72ms/step - loss: -1901600768000.0000 - reconstruction_loss: -1708316819456.0000 - kl_loss: 301637824.0000\n",
      "Epoch 420/500\n",
      "23/23 [==============================] - 2s 102ms/step - loss: -1868609858218.6667 - reconstruction_loss: -2281365569536.0000 - kl_loss: 420183296.0000\n",
      "Epoch 421/500\n",
      "23/23 [==============================] - 2s 79ms/step - loss: -2566945286826.6665 - reconstruction_loss: -2512170254336.0000 - kl_loss: 532194528.0000\n",
      "Epoch 422/500\n",
      "23/23 [==============================] - 2s 70ms/step - loss: -3013117850965.3335 - reconstruction_loss: -3025076748288.0000 - kl_loss: 658838912.0000\n",
      "Epoch 423/500\n",
      "23/23 [==============================] - 1s 49ms/step - loss: -3354923444906.6665 - reconstruction_loss: -3823695298560.0000 - kl_loss: 765448192.0000\n",
      "Epoch 424/500\n",
      "23/23 [==============================] - 1s 46ms/step - loss: -3807205654528.0000 - reconstruction_loss: -4383558598656.0000 - kl_loss: 1106015744.0000\n",
      "Epoch 425/500\n",
      "23/23 [==============================] - 1s 45ms/step - loss: -5413733990400.0000 - reconstruction_loss: -5316879056896.0000 - kl_loss: 1382737280.0000\n",
      "Epoch 426/500\n",
      "23/23 [==============================] - 1s 53ms/step - loss: -6120569077760.0000 - reconstruction_loss: -6589594468352.0000 - kl_loss: 1813733888.0000\n",
      "Epoch 427/500\n",
      "23/23 [==============================] - 1s 48ms/step - loss: -6697101003434.6670 - reconstruction_loss: -7907983753216.0000 - kl_loss: 2237323776.0000\n",
      "Epoch 428/500\n",
      "23/23 [==============================] - 1s 62ms/step - loss: -9921805811712.0000 - reconstruction_loss: -9303655186432.0000 - kl_loss: 2924050432.0000\n",
      "Epoch 429/500\n",
      "23/23 [==============================] - 1s 45ms/step - loss: -11704818554197.3340 - reconstruction_loss: -12462070956032.0000 - kl_loss: 3479723264.0000\n",
      "Epoch 430/500\n",
      "23/23 [==============================] - 1s 46ms/step - loss: -12492566473386.6660 - reconstruction_loss: -13567629721600.0000 - kl_loss: 4973526016.0000\n",
      "Epoch 431/500\n",
      "23/23 [==============================] - 1s 47ms/step - loss: -16473846666581.3340 - reconstruction_loss: -15718715228160.0000 - kl_loss: 5899456000.0000\n",
      "Epoch 432/500\n",
      "23/23 [==============================] - 1s 53ms/step - loss: -16158831585962.6660 - reconstruction_loss: -19705184124928.0000 - kl_loss: 6813044224.0000\n",
      "Epoch 433/500\n",
      "23/23 [==============================] - 1s 54ms/step - loss: -22577951801344.0000 - reconstruction_loss: -21845315158016.0000 - kl_loss: 9621067776.0000\n",
      "Epoch 434/500\n",
      "23/23 [==============================] - 1s 50ms/step - loss: -24794200364373.3320 - reconstruction_loss: -26430454890496.0000 - kl_loss: 10333911040.0000\n",
      "Epoch 435/500\n",
      "23/23 [==============================] - 1s 44ms/step - loss: -31877413295445.3320 - reconstruction_loss: -30109576724480.0000 - kl_loss: 12559083520.0000\n",
      "Epoch 436/500\n",
      "23/23 [==============================] - 1s 43ms/step - loss: -35144946767189.3320 - reconstruction_loss: -38691963666432.0000 - kl_loss: 16585009152.0000\n",
      "Epoch 437/500\n",
      "23/23 [==============================] - 1s 42ms/step - loss: -38679833564501.3359 - reconstruction_loss: -40352283099136.0000 - kl_loss: 21436116992.0000\n",
      "Epoch 438/500\n",
      "23/23 [==============================] - 1s 41ms/step - loss: -47181128881493.3359 - reconstruction_loss: -47189355134976.0000 - kl_loss: 21991647232.0000\n",
      "Epoch 439/500\n",
      "23/23 [==============================] - 1s 41ms/step - loss: -57516971698858.6641 - reconstruction_loss: -55031554375680.0000 - kl_loss: 27730395136.0000\n",
      "Epoch 440/500\n",
      "23/23 [==============================] - 1s 41ms/step - loss: -53446431561045.3359 - reconstruction_loss: -62611387318272.0000 - kl_loss: 31624122368.0000\n",
      "Epoch 441/500\n",
      "23/23 [==============================] - 1s 48ms/step - loss: -71698903203840.0000 - reconstruction_loss: -69106053152768.0000 - kl_loss: 39184773120.0000\n",
      "Epoch 442/500\n",
      "23/23 [==============================] - 1s 47ms/step - loss: -70494768857088.0000 - reconstruction_loss: -79959095771136.0000 - kl_loss: 43807199232.0000\n",
      "Epoch 443/500\n",
      "23/23 [==============================] - 1s 48ms/step - loss: -72202597171200.0000 - reconstruction_loss: -92540816588800.0000 - kl_loss: 50453819392.0000\n",
      "Epoch 444/500\n",
      "23/23 [==============================] - 1s 44ms/step - loss: -96842315988992.0000 - reconstruction_loss: -102776059199488.0000 - kl_loss: 72311791616.0000\n",
      "Epoch 445/500\n",
      "23/23 [==============================] - 1s 44ms/step - loss: -106647944254805.3281 - reconstruction_loss: -116198998016000.0000 - kl_loss: 66103615488.0000\n",
      "Epoch 446/500\n",
      "23/23 [==============================] - 1s 44ms/step - loss: -121977238978560.0000 - reconstruction_loss: -127981611646976.0000 - kl_loss: 86994739200.0000\n",
      "Epoch 447/500\n",
      "23/23 [==============================] - 1s 43ms/step - loss: -152353627613866.6562 - reconstruction_loss: -144076070453248.0000 - kl_loss: 91617271808.0000\n",
      "Epoch 448/500\n",
      "23/23 [==============================] - 1s 44ms/step - loss: -159554697778517.3438 - reconstruction_loss: -163970979725312.0000 - kl_loss: 111020802048.0000\n",
      "Epoch 449/500\n",
      "23/23 [==============================] - 1s 44ms/step - loss: -174602721776981.3438 - reconstruction_loss: -184627708821504.0000 - kl_loss: 133005639680.0000\n",
      "Epoch 450/500\n",
      "23/23 [==============================] - 1s 47ms/step - loss: -213877901317461.3438 - reconstruction_loss: -203850791256064.0000 - kl_loss: 145523441664.0000\n",
      "Epoch 451/500\n",
      "23/23 [==============================] - 1s 52ms/step - loss: -230082732534442.6562 - reconstruction_loss: -248647165935616.0000 - kl_loss: 180027162624.0000\n",
      "Epoch 452/500\n",
      "23/23 [==============================] - 1s 60ms/step - loss: -270196213634389.3438 - reconstruction_loss: -255324162359296.0000 - kl_loss: 213897904128.0000\n",
      "Epoch 453/500\n",
      "23/23 [==============================] - 2s 66ms/step - loss: -282283563024384.0000 - reconstruction_loss: -283169408417792.0000 - kl_loss: 214681501696.0000\n",
      "Epoch 454/500\n",
      "23/23 [==============================] - 1s 51ms/step - loss: -378995609348778.6875 - reconstruction_loss: -329735510949888.0000 - kl_loss: 287094571008.0000\n",
      "Epoch 455/500\n",
      "23/23 [==============================] - 1s 44ms/step - loss: -292396661211136.0000 - reconstruction_loss: -348255544147968.0000 - kl_loss: 270187347968.0000\n",
      "Epoch 456/500\n",
      "23/23 [==============================] - 1s 49ms/step - loss: -428435144267093.3125 - reconstruction_loss: -383781466800128.0000 - kl_loss: 369520705536.0000\n",
      "Epoch 457/500\n",
      "23/23 [==============================] - 1s 50ms/step - loss: -383135265761962.6875 - reconstruction_loss: -416054891249664.0000 - kl_loss: 326942228480.0000\n",
      "Epoch 458/500\n",
      "23/23 [==============================] - 1s 51ms/step - loss: -436764829286400.0000 - reconstruction_loss: -480954934099968.0000 - kl_loss: 466473517056.0000\n",
      "Epoch 459/500\n",
      "23/23 [==============================] - 1s 54ms/step - loss: -427976067208533.3125 - reconstruction_loss: -495144331640832.0000 - kl_loss: 427424186368.0000\n",
      "Epoch 460/500\n",
      "23/23 [==============================] - 1s 52ms/step - loss: -513412738755242.6875 - reconstruction_loss: -565664305643520.0000 - kl_loss: 513922793472.0000\n",
      "Epoch 461/500\n",
      "23/23 [==============================] - 1s 56ms/step - loss: -602171322138624.0000 - reconstruction_loss: -586376986755072.0000 - kl_loss: 615668842496.0000\n",
      "Epoch 462/500\n",
      "23/23 [==============================] - 1s 55ms/step - loss: -686044306123434.6250 - reconstruction_loss: -681254559154176.0000 - kl_loss: 592977264640.0000\n",
      "Epoch 463/500\n",
      "23/23 [==============================] - 1s 64ms/step - loss: -619065157965141.3750 - reconstruction_loss: -713673911828480.0000 - kl_loss: 724793622528.0000\n",
      "Epoch 464/500\n",
      "23/23 [==============================] - 1s 56ms/step - loss: -688633150788949.3750 - reconstruction_loss: -752659195756544.0000 - kl_loss: 766566924288.0000\n",
      "Epoch 465/500\n",
      "23/23 [==============================] - 1s 51ms/step - loss: -770791518677674.6250 - reconstruction_loss: -822621126000640.0000 - kl_loss: 784174612480.0000\n",
      "Epoch 466/500\n",
      "23/23 [==============================] - 1s 51ms/step - loss: -778760883623253.3750 - reconstruction_loss: -871870710677504.0000 - kl_loss: 971968610304.0000\n",
      "Epoch 467/500\n",
      "23/23 [==============================] - 1s 54ms/step - loss: -903726304154965.3750 - reconstruction_loss: -959436973670400.0000 - kl_loss: 941969637376.0000\n",
      "Epoch 468/500\n",
      "23/23 [==============================] - 1s 63ms/step - loss: -1146705533730816.0000 - reconstruction_loss: -1009323085922304.0000 - kl_loss: 1192050819072.0000\n",
      "Epoch 469/500\n",
      "23/23 [==============================] - 2s 68ms/step - loss: -1070345690458794.6250 - reconstruction_loss: -1103860349272064.0000 - kl_loss: 1105908989952.0000\n",
      "Epoch 470/500\n",
      "23/23 [==============================] - 2s 69ms/step - loss: -1116730285883392.0000 - reconstruction_loss: -1202077191634944.0000 - kl_loss: 1300283392000.0000\n",
      "Epoch 471/500\n",
      "23/23 [==============================] - 2s 71ms/step - loss: -1446318006337536.0000 - reconstruction_loss: -1276513269841920.0000 - kl_loss: 1409147731968.0000\n",
      "Epoch 472/500\n",
      "23/23 [==============================] - 1s 61ms/step - loss: -1466668019963221.2500 - reconstruction_loss: -1377287832338432.0000 - kl_loss: 1593098371072.0000\n",
      "Epoch 473/500\n",
      "23/23 [==============================] - 2s 68ms/step - loss: -1622748132540416.0000 - reconstruction_loss: -1514593507606528.0000 - kl_loss: 1866507616256.0000\n",
      "Epoch 474/500\n",
      "23/23 [==============================] - 1s 63ms/step - loss: -1409770010094250.7500 - reconstruction_loss: -1587806459658240.0000 - kl_loss: 1674533404672.0000\n",
      "Epoch 475/500\n",
      "23/23 [==============================] - 1s 63ms/step - loss: -1776293722281301.2500 - reconstruction_loss: -1802354034737152.0000 - kl_loss: 2269793222656.0000\n",
      "Epoch 476/500\n",
      "23/23 [==============================] - 2s 67ms/step - loss: -1837275625291776.0000 - reconstruction_loss: -1817039333228544.0000 - kl_loss: 2172070264832.0000\n",
      "Epoch 477/500\n",
      "23/23 [==============================] - 1s 63ms/step - loss: -1795853775099221.2500 - reconstruction_loss: -1971112292384768.0000 - kl_loss: 2401843806208.0000\n",
      "Epoch 478/500\n",
      "23/23 [==============================] - 1s 64ms/step - loss: -2268932466191018.5000 - reconstruction_loss: -2072223338725376.0000 - kl_loss: 2532800200704.0000\n",
      "Epoch 479/500\n",
      "23/23 [==============================] - 1s 62ms/step - loss: -1907370842128384.0000 - reconstruction_loss: -2398935192698880.0000 - kl_loss: 2914106736640.0000\n",
      "Epoch 480/500\n",
      "23/23 [==============================] - 2s 69ms/step - loss: -2219627885800106.7500 - reconstruction_loss: -2473235711000576.0000 - kl_loss: 3088418078720.0000\n",
      "Epoch 481/500\n",
      "23/23 [==============================] - 2s 81ms/step - loss: -2334726376740181.5000 - reconstruction_loss: -2750153190539264.0000 - kl_loss: 3719545225216.0000\n",
      "Epoch 482/500\n",
      "23/23 [==============================] - 3s 116ms/step - loss: -2795793313781077.5000 - reconstruction_loss: -2844720585768960.0000 - kl_loss: 3877335465984.0000\n",
      "Epoch 483/500\n",
      "23/23 [==============================] - 1s 64ms/step - loss: -2524628338955605.5000 - reconstruction_loss: -2826037746466816.0000 - kl_loss: 3563350130688.0000\n",
      "Epoch 484/500\n",
      "23/23 [==============================] - 1s 63ms/step - loss: -2705154180644864.0000 - reconstruction_loss: -3033101609467904.0000 - kl_loss: 3864735776768.0000\n",
      "Epoch 485/500\n",
      "23/23 [==============================] - 1s 60ms/step - loss: -3140304356682410.5000 - reconstruction_loss: -3213391183216640.0000 - kl_loss: 4369923440640.0000\n",
      "Epoch 486/500\n",
      "23/23 [==============================] - 1s 64ms/step - loss: -3571797122001578.5000 - reconstruction_loss: -3337712769695744.0000 - kl_loss: 4660153286656.0000\n",
      "Epoch 487/500\n",
      "23/23 [==============================] - 2s 87ms/step - loss: -3916019401053525.5000 - reconstruction_loss: -3609811027492864.0000 - kl_loss: 4657760960512.0000\n",
      "Epoch 488/500\n",
      "23/23 [==============================] - 2s 67ms/step - loss: -3570031129875797.5000 - reconstruction_loss: -3861208079794176.0000 - kl_loss: 5153541849088.0000\n",
      "Epoch 489/500\n",
      "23/23 [==============================] - 2s 66ms/step - loss: -4734949942187349.0000 - reconstruction_loss: -3945340851978240.0000 - kl_loss: 6157729005568.0000\n",
      "Epoch 490/500\n",
      "23/23 [==============================] - 1s 62ms/step - loss: -5103072405618688.0000 - reconstruction_loss: -4204203094310912.0000 - kl_loss: 5777443520512.0000\n",
      "Epoch 491/500\n",
      "23/23 [==============================] - 1s 61ms/step - loss: -4351620185150805.5000 - reconstruction_loss: -4435183482699776.0000 - kl_loss: 6083324674048.0000\n",
      "Epoch 492/500\n",
      "23/23 [==============================] - 2s 70ms/step - loss: -4560254703151787.0000 - reconstruction_loss: -4635156086259712.0000 - kl_loss: 6359977820160.0000\n",
      "Epoch 493/500\n",
      "23/23 [==============================] - 2s 74ms/step - loss: -5066510098410155.0000 - reconstruction_loss: -4908708995792896.0000 - kl_loss: 7297897594880.0000\n",
      "Epoch 494/500\n",
      "23/23 [==============================] - 2s 95ms/step - loss: -5293883184032427.0000 - reconstruction_loss: -5126923399200768.0000 - kl_loss: 7129266651136.0000\n",
      "Epoch 495/500\n",
      "23/23 [==============================] - 2s 88ms/step - loss: -5387959778585259.0000 - reconstruction_loss: -5469546362175488.0000 - kl_loss: 7574225158144.0000\n",
      "Epoch 496/500\n",
      "23/23 [==============================] - 1s 60ms/step - loss: -5102869199978496.0000 - reconstruction_loss: -5740650603479040.0000 - kl_loss: 8617227452416.0000\n",
      "Epoch 497/500\n",
      "23/23 [==============================] - 1s 62ms/step - loss: -6730624056426496.0000 - reconstruction_loss: -6026205262249984.0000 - kl_loss: 9257379430400.0000\n",
      "Epoch 498/500\n",
      "23/23 [==============================] - 2s 66ms/step - loss: -6542808089362432.0000 - reconstruction_loss: -6256673207353344.0000 - kl_loss: 9079490609152.0000\n",
      "Epoch 499/500\n",
      "23/23 [==============================] - 1s 60ms/step - loss: -6838360045081941.0000 - reconstruction_loss: -6706610760056832.0000 - kl_loss: 9629045096448.0000\n",
      "Epoch 500/500\n",
      "23/23 [==============================] - 2s 76ms/step - loss: -7398061213657771.0000 - reconstruction_loss: -6964606761172992.0000 - kl_loss: 10857137307648.0000\n"
     ]
    }
   ],
   "source": [
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=keras.optimizers.Adam())\n",
    "history = vae.fit(data, epochs=500, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "298bd106-e47e-4aa6-88da-a435ce09834d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "row = 1\n",
    "col = 3\n",
    "fig, ax = plt.subplots(row, col, figsize=(15,4))\n",
    "ax[0].plot(history.history[\"loss\"])\n",
    "ax[0].set_ylabel('loss')\n",
    "ax[0].set_xlabel('epoch')\n",
    "ax[1].plot(history.history[\"reconstruction_loss\"])\n",
    "ax[1].set_ylabel('reconstruction_loss')\n",
    "ax[1].set_xlabel('epoch')\n",
    "ax[2].plot(history.history[\"kl_loss\"])\n",
    "ax[2].set_ylabel('kl_loss')\n",
    "ax[2].set_xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afc885e0-927d-4da2-9727-015e2f484a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2022年以降を検証データにする\n",
    "funds_valid= fundsdata.query('date >= 20200101')\n",
    "valid_dif = funds_valid[ ['jp_stock','foreign_stock','emerging_stock','jp_bond','foreign_bond','emerging_bond','jp_reit','foreign_reit'] ]\n",
    "valid_dif.reset_index(inplace=True, drop=True)\n",
    "funds_dif = np.log(funds_dif)-np.log(funds_dif.shift(1))\n",
    "valid_dif = valid_dif.loc[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6848176-1542-4d30-8f41-ae6a1fd63bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ウィンドウ区間の変化率をndarrayに変換\n",
    "vlist = []\n",
    "for i in range(len(valid_dif.index) - w):\n",
    "    vlist.append(valid_dif.loc[i+1:i+w].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7daa195-8201-4629-9f11-93f73e9fa1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_valid = np.stack(vlist,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d8f91d3-794a-4ead-be43-eb3310271eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 6ms/step\n",
      "23/23 [==============================] - 0s 9ms/step\n"
     ]
    }
   ],
   "source": [
    "# vae = VAE(encoder, decoder)\n",
    "# vae.compile(optimizer=keras.optimizers.Adam())\n",
    "# vae.built = True\n",
    "# vae.load_weights(\"vae.h5\")\n",
    "\n",
    "x_input = np.expand_dims(x_valid,-1)\n",
    "x_decode = vae.predict(x_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2535a17-8ccd-420f-ae6f-0c01b5ac3a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_decode[0,:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "882d2ceb-15b1-4516-84e1-cab3df857375",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "in user code:\n\n    File \"/home/nske/miniconda3/envs/jp38/lib/python3.8/site-packages/keras/engine/training.py\", line 2137, in predict_function  *\n        return step_function(self, iterator)\n    File \"/home/nske/miniconda3/envs/jp38/lib/python3.8/site-packages/keras/engine/training.py\", line 2123, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/nske/miniconda3/envs/jp38/lib/python3.8/site-packages/keras/engine/training.py\", line 2111, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/home/nske/miniconda3/envs/jp38/lib/python3.8/site-packages/keras/engine/training.py\", line 2079, in predict_step\n        return self(x, training=False)\n    File \"/home/nske/miniconda3/envs/jp38/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/nske/miniconda3/envs/jp38/lib/python3.8/site-packages/keras/engine/training.py\", line 588, in call\n        raise NotImplementedError(\n\n    NotImplementedError: Exception encountered when calling layer 'vae' (type VAE).\n    \n    Unimplemented `tf.keras.Model.call()`: if you intend to create a `Model` with the Functional API, please provide `inputs` and `outputs` arguments. Otherwise, subclass `Model` with an overridden `call()` method.\n    \n    Call arguments received by layer 'vae' (type VAE):\n      • inputs=tf.Tensor(shape=(None, 8), dtype=float32)\n      • training=False\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1282/515256498.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/jp38/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/jp38/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__predict_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: in user code:\n\n    File \"/home/nske/miniconda3/envs/jp38/lib/python3.8/site-packages/keras/engine/training.py\", line 2137, in predict_function  *\n        return step_function(self, iterator)\n    File \"/home/nske/miniconda3/envs/jp38/lib/python3.8/site-packages/keras/engine/training.py\", line 2123, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/nske/miniconda3/envs/jp38/lib/python3.8/site-packages/keras/engine/training.py\", line 2111, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/home/nske/miniconda3/envs/jp38/lib/python3.8/site-packages/keras/engine/training.py\", line 2079, in predict_step\n        return self(x, training=False)\n    File \"/home/nske/miniconda3/envs/jp38/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/nske/miniconda3/envs/jp38/lib/python3.8/site-packages/keras/engine/training.py\", line 588, in call\n        raise NotImplementedError(\n\n    NotImplementedError: Exception encountered when calling layer 'vae' (type VAE).\n    \n    Unimplemented `tf.keras.Model.call()`: if you intend to create a `Model` with the Functional API, please provide `inputs` and `outputs` arguments. Otherwise, subclass `Model` with an overridden `call()` method.\n    \n    Call arguments received by layer 'vae' (type VAE):\n      • inputs=tf.Tensor(shape=(None, 8), dtype=float32)\n      • training=False\n      • mask=None\n"
     ]
    }
   ],
   "source": [
    "out=vae.predict(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9bac7d2c-b212-4e43-8233-959e27f7aecd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/nske/miniconda3/envs/jp38/lib/python3.8/site-packages/keras/engine/training.py\", line 2137, in predict_function  *\n        return step_function(self, iterator)\n    File \"/home/nske/miniconda3/envs/jp38/lib/python3.8/site-packages/keras/engine/training.py\", line 2123, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/nske/miniconda3/envs/jp38/lib/python3.8/site-packages/keras/engine/training.py\", line 2111, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/home/nske/miniconda3/envs/jp38/lib/python3.8/site-packages/keras/engine/training.py\", line 2079, in predict_step\n        return self(x, training=False)\n    File \"/home/nske/miniconda3/envs/jp38/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/nske/miniconda3/envs/jp38/lib/python3.8/site-packages/keras/engine/input_spec.py\", line 216, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Layer \"decoder\" expects 1 input(s), but it received 3 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 2) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(None, 2) dtype=float32>, <tf.Tensor 'IteratorGetNext:2' shape=(None, 2) dtype=float32>]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1282/3549286175.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/jp38/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/jp38/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__predict_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/nske/miniconda3/envs/jp38/lib/python3.8/site-packages/keras/engine/training.py\", line 2137, in predict_function  *\n        return step_function(self, iterator)\n    File \"/home/nske/miniconda3/envs/jp38/lib/python3.8/site-packages/keras/engine/training.py\", line 2123, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/nske/miniconda3/envs/jp38/lib/python3.8/site-packages/keras/engine/training.py\", line 2111, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/home/nske/miniconda3/envs/jp38/lib/python3.8/site-packages/keras/engine/training.py\", line 2079, in predict_step\n        return self(x, training=False)\n    File \"/home/nske/miniconda3/envs/jp38/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/nske/miniconda3/envs/jp38/lib/python3.8/site-packages/keras/engine/input_spec.py\", line 216, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Layer \"decoder\" expects 1 input(s), but it received 3 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 2) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(None, 2) dtype=float32>, <tf.Tensor 'IteratorGetNext:2' shape=(None, 2) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "deout=vae.decoder.predict(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09cb929-7a4d-40d0-a1e9-ff606742d05f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
